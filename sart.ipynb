{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All Imports\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "from numba import cuda\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import glob\n",
    "from skimage import filters\n",
    "from skimage.filters import unsharp_mask, threshold_local, threshold_minimum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "\n",
    "\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import label\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "#from scipy.misc import imresize\n",
    "import pywt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline  \n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "import pywt\n",
    "#import hdf5storage\n",
    "\n",
    "import scipy.io as sio\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "#import pylidc as pl\n",
    "\n",
    "\n",
    "import pywt\n",
    "import numpy as np\n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import skimage.io as io\n",
    "#from sklearn.decomposition import PCA\n",
    "import collections, numpy\n",
    "import warnings\n",
    "from scipy import ndimage, misc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import pymrt as mrt\n",
    "#import pymrt.geometry\n",
    "#import ipyvolume as ipv\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import numpy as np\n",
    "from ipdb import set_trace as bp\n",
    "\n",
    "\n",
    "\n",
    "#from image_gen import ImageDataGenerator\n",
    "#from load_data import loadDataMontgomery, loadDataJSRT\n",
    "#from build_model import build_UNet2D_4L\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import numpy\n",
    "import warnings\n",
    "import functools\n",
    "import pickle\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     2,
     32,
     91,
     138,
     143,
     172
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Reading Code\n",
    "\n",
    "def get_duke_proj(index, lesion):\n",
    "    if lesion:\n",
    "        a       = sio.loadmat(\"/media/dril/My Passport/CHO-DATA/WITH-LESION-MAT/LE/\"+str(index)+\".mat\")[\"head\"]\n",
    "    else:\n",
    "        a       = sio.loadmat(\"/media/dril/My Passport/CHO-DATA/NO-LESION-MAT/LE/\"+str(index)+\".mat\")[\"head\"]\n",
    "    testvol = np.rollaxis(a, 2, 0)\n",
    "    testvol = np.moveaxis(testvol, [0, 1, 2], [0, 2, 1])\n",
    "    \n",
    "    testvol[testvol == 0.80] = 0.81\n",
    "    print(np.unique(testvol.flatten()))\n",
    "    \n",
    "    testvol = testvol/65\n",
    "    proj_arr         = W*testvol\n",
    "    \n",
    "    # All Flags\n",
    "    insert_noise     = 1\n",
    "    if insert_noise:\n",
    "        I0        = 1000\n",
    "        proj      = I0*np.exp(-proj_arr)\n",
    "        proj_noi  = np.random.poisson(proj)\n",
    "        proj_noi[proj_noi == 0] = 1\n",
    "        g_noi                   = np.log(I0) - np.log(proj_noi) # convert back to line integrals \n",
    "        g_noi[g_noi < 0]        = 0\n",
    "        proj_arr = g_noi\n",
    "    \n",
    "    temp_proj = np.reshape(proj_arr, [detCols, num_angles, detRows])\n",
    "    temp_proj = np.rollaxis(temp_proj, 0, 2)\n",
    "    #print(temp_proj.shape)\n",
    "    return temp_proj\n",
    "\n",
    "def load_prj_raw(breast_type):\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    print(BINSx*BINSy, ANGLES, prj_allangle.shape)\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    start_angle = -21.30\n",
    "    \n",
    "    proj_paths = glob.glob(\"/media/dril/Windows/mcgpu1/projections_without_fsm\"+projection_name+\"/*.raw\")\n",
    "    proj_paths.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    \n",
    "    for p in proj_paths:\n",
    "        print(p)\n",
    "        \n",
    "        if '.0000.' in p:\n",
    "            continue\n",
    "        \n",
    "        a    = np.fromfile(p, dtype='float32')#pydicom.dcmread(p)\n",
    "        a    = np.reshape(a, [2, 1504, 3000])\n",
    "        temp = a[0, :, :]#.pixel_array.T\n",
    "        temp = np.log(10000000)-np.log(temp)\n",
    "        \n",
    "        if(0):\n",
    "            # Sharpening filter\n",
    "            temp = unsharp_mask(temp, radius=3, amount=1, preserve_range=True)\n",
    "            thresh_min      = threshold_minimum(temp)\n",
    "            binary_adaptive = temp > thresh_min\n",
    "            temp = np.multiply(temp, binary_adaptive)\n",
    "            \n",
    "        if breast_type == \"right\":\n",
    "            temp = np.fliplr(temp)\n",
    "            temp = temp[-BINSy:]\n",
    "        else:\n",
    "            temp = temp[:BINSy]\n",
    "            temp = np.flipud(temp)\n",
    "            print(temp.shape)\n",
    "        \n",
    "        temp = temp.flatten()\n",
    "        temp = x_y_flip(temp)\n",
    "        \n",
    "        x.append(temp)\n",
    "        y.append(start_angle)\n",
    "        start_angle = start_angle + 1.92\n",
    "    \n",
    "    y = np.array(y)*np.pi/180\n",
    "    \n",
    "    print(\"length x \", len(x), \" \", len(y))\n",
    "    y, x = zip(*sorted(zip(y, x)))\n",
    "    for j in range(len(x)):\n",
    "        print(\"Proj \", j)\n",
    "        flag2 = j\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = x[j][i]\n",
    "    \n",
    "    return prj_allangle, y\n",
    "\n",
    "def load_prj_ima(breast_type):\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    print(BINSx*BINSy, ANGLES, prj_allangle.shape)\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    proj_paths = glob.glob(\"/media/dril/ubuntudata/DBT_recon_data/\"+projection_name+\"/CE*.IMA\")\n",
    "    for p in proj_paths:\n",
    "        if '.0000.' in p:\n",
    "            continue\n",
    "        \n",
    "        a    = pydicom.dcmread(p)\n",
    "        temp = a.pixel_array.T\n",
    "        temp = np.log(10000)-np.log(temp)\n",
    "        # Sharpening filter\n",
    "        temp = unsharp_mask(temp, radius=3, amount=1, preserve_range=True)\n",
    "        thresh_min      = threshold_minimum(temp)\n",
    "        binary_adaptive = temp > thresh_min\n",
    "        temp = np.multiply(temp, binary_adaptive)\n",
    "            \n",
    "        if breast_type == \"right\":\n",
    "            temp = np.fliplr(temp)\n",
    "            temp = temp[-BINSy:]\n",
    "        else:\n",
    "            temp = temp[:BINSy]\n",
    "            temp = np.flipud(temp)\n",
    "        \n",
    "        temp = temp.flatten()\n",
    "        temp = x_y_flip(temp)\n",
    "        \n",
    "        x.append(temp)\n",
    "        y.append(float(a[0x00181530].value))\n",
    "    y = np.array(y)*np.pi/180\n",
    "    \n",
    "    print(\"length x \", len(x), \" \", len(y))\n",
    "    y, x = zip(*sorted(zip(y, x)))\n",
    "    for j in range(len(x)):\n",
    "        print(\"Proj \", j)\n",
    "        flag2 = j\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = x[j][i]\n",
    "    \n",
    "    return prj_allangle, y\n",
    "\n",
    "def import_param():\n",
    "    for i in range(ANGLES):\n",
    "        index[i] = i\n",
    "    return index\n",
    "\n",
    "def load_prj():\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    \n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    scat_allangle = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    for viewangle in range(ANGLES):\n",
    "        s        = viewangle + 1\n",
    "        filename = basepath + filepath+str(s).zfill(4)#+'.raw'\n",
    "        \n",
    "        with open(filename, 'rb') as f:\n",
    "            primary_plus_scatter  = np.fromfile(f, dtype=np.float32)\n",
    "            host_prj_temp1        = primary_plus_scatter[:b_size]\n",
    "            host_prj_temp2        = primary_plus_scatter[b_size:]\n",
    "        \n",
    "        host_prj_1view_temp = x_y_flip(host_prj_temp1)\n",
    "        host_sct_1view_temp = x_y_flip(host_prj_temp2)\n",
    "        \n",
    "        print(host_prj_1view_temp.shape)\n",
    "        \n",
    "        # all angle together\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = host_prj_1view_temp[i]\n",
    "            scat_allangle[flag2*BINSx*BINSy + i] = host_sct_1view_temp[i]\n",
    "        \n",
    "        flag2 = flag2+1\n",
    "    return prj_allangle, scat_allangle\n",
    "    \n",
    "def load_prj_std(data_type):\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    \n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    #scat_allangle = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    for viewangle in range(ANGLES):\n",
    "        s        = viewangle + 1\n",
    "        \n",
    "        if data_type   == 0:\n",
    "            #filename = basepath + 'OSTR_prelog/'+projection_name+str(s).zfill(4)#+\".raw\"\n",
    "            filename = basepath + 'Projections_Renamed_Seg/'+projection_name+str(s).zfill(4)#+\".raw\"\n",
    "        elif data_type == 1:\n",
    "            filename = basepath + 'OSTR_scatter/'+scatter_name+str(s).zfill(4)#+\".raw\"\n",
    "        else:\n",
    "            filename = basepath + 'OSTR_blank/'+blank_name+str(s).zfill(4)#+\".raw\"\n",
    "        \n",
    "        #print(filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            #data  = np.load(f)\n",
    "            data  = np.fromfile(f, dtype=np.float32)\n",
    "            # If doign SART\n",
    "            #data  = np.log(10000) - np.log(data)\n",
    "            \n",
    "            #print(data.shape)\n",
    "            #data  = np.reshape(data, (1400, 3584))\n",
    "            #data  = data[:1400, :]\n",
    "            #data  = np.flip(data, 0)\n",
    "            #data  = data.flatten()\n",
    "            #print(data.shape)\n",
    "                #data  = np.fromfile(f, dtype=np.float32)\n",
    "        #np.save(filename+'.npy', data)\n",
    "        proj = x_y_flip(data)\n",
    "        \n",
    "        # all angle together\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = proj[i]\n",
    "        \n",
    "        flag2 = flag2+1\n",
    "    \n",
    "    return prj_allangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     2,
     55,
     161,
     261,
     371,
     479,
     584,
     686,
     727,
     754,
     856,
     1017,
     1178,
     1341,
     1504,
     1667,
     1830,
     1993,
     2189,
     2339,
     2489,
     2639
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] CNN Models\n",
    "\n",
    "def rating_cnn(input_size = (256, 256, 1)):\n",
    "    filter1 = 4\n",
    "    filter2 = 4\n",
    "    filter3 = 4\n",
    "    filter4 = 4\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same',kernel_initializer = 'glorot_normal')(input1)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #conv1 = Dropout(0.2)(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #conv2 = Dropout(0.2)(conv2)\n",
    "    \n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #conv2 = Dropout(0.2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filter3, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #conv3 = Dropout(0.2)(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(filter4, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    out1 = Flatten()(conv4)\n",
    "    #out1 = Dropout(0.3)(out1)\n",
    "    \n",
    "    #out2 = Dense(64, kernel_initializer = 'glorot_normal')(out1)\n",
    "    #out2 = LeakyReLU()(out2)\n",
    "    #out2 = Dropout(0.3)(out2)\n",
    "    \n",
    "    #out3 = Dense(32,  kernel_initializer = 'glorot_normal')(out2)\n",
    "    #out3 = LeakyReLU()(out3)\n",
    "    \n",
    "    out3 = Dense(1, activation=\"sigmoid\")(out1)\n",
    "    \n",
    "    model  = Model(input = input1, output = out3)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def unet_double(pretrained_weights = None, input_size = (128, 128, 5), ):\n",
    "    filter1 = 64\n",
    "    filter2 = 128\n",
    "    filter3 = 256\n",
    "    filter4 = 512\n",
    "    filter5 = 1024\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    input3 = Input((1, ))\n",
    "    \n",
    "    hash_val1 = Dense(128, activation='relu')(input2)\n",
    "    hash_val1 = Dense(1, activation='relu')(hash_val1)\n",
    "    \n",
    "    hash_val2 = Dense(128, activation='relu')(input3)\n",
    "    hash_val2 = Dense(1, activation='relu')(hash_val2)\n",
    "    \n",
    "    hash_val = Multiply()([hash_val1, hash_val2])\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(hash_val)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2, input3], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_vanilla(pretrained_weights = None, input_size = (128, 128, 1), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    #input2 = Input((1, ))\n",
    "    \n",
    "    #hash_val = Dense(128, activation='relu')(input2)\n",
    "    #hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    #hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    #input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    #conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    #conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    #conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.1)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    #pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.1)(conv5)\n",
    "    #drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    #conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    #conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    #conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = input1, output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_two(pretrained_weights = None, input_size = (128, 128, 1), ):\n",
    "    filter1 = 64\n",
    "    filter2 = 128\n",
    "    filter3 = 256\n",
    "    filter4 = 512\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(input2)\n",
    "    hash_val = Dropout(0.3)(hash_val)\n",
    "    hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    hash_val1 = Dense(128, activation='relu')(input2)\n",
    "    hash_val1 = Dropout(0.3)(hash_val1)\n",
    "    hash_val1 = Dense(32, activation='relu')(hash_val1)\n",
    "    hash_val1 = Dense(1, activation='relu')(hash_val1)\n",
    "    \n",
    "    input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Dropout(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val1, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Dropout(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val1, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Dropout(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val1, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet(pretrained_weights = None, input_size = (256, 256, 1), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(input2)\n",
    "    hash_val = Dropout(0.3)(hash_val)\n",
    "    hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4, up6], axis = 3)\n",
    "    \n",
    "    \n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Dropout(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Dropout(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Dropout(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_no_dense(pretrained_weights = None, input_size = (256, 256, 1), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    \n",
    "    #hash_val = Dense(128, activation='relu')(input2)\n",
    "    #hash_val = Dropout(0.3)(hash_val)\n",
    "    #hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    hash_val = input2#Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Dropout(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Dropout(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Dropout(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_lstm(pretrained_weights = None, input_size = (128, 128, 2), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    single_input = Input(input_size)\n",
    "    \n",
    "    input1 = Lambda(lambda x: x[:, :, :, 0])(single_input)\n",
    "    input1 = Reshape([128, 128, 1])(input1)\n",
    "    \n",
    "    input2 = Lambda(lambda x: x[:, :, :, 1])(single_input)\n",
    "    input2 = Flatten()(input2)\n",
    "    input2 = Reshape([1, 128*128])(input2) \n",
    "    input2 = Lambda(lambda x: x[:, :, 0])(input2)\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(input2)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "        \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #conv10 = Subtract()([input1, conv9])\n",
    "    \n",
    "    model  = Model(input = single_input, output = conv9)\n",
    "    #model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def lstm_model():\n",
    "    input_size = (5, 128, 128, 1)\n",
    "    \n",
    "    input1     = Input(input_size)\n",
    "    input2     = Input((1, ))\n",
    "    \n",
    "    cnn_model  = unet()\n",
    "    \n",
    "    #time2       = TimeDistributed(cnn_model)(input1)\n",
    "    \n",
    "    lstm_out1   = ConvLSTM2D(filters=32, kernel_size=(3, 3), input_shape=(5, 128, 128, 1),\n",
    "                             padding='same', activation=LeakyReLU(alpha=0.2), return_sequences=True)(input1)\n",
    "    \n",
    "    lstm_out2   = ConvLSTM2D(filters=1, kernel_size=(3, 3), padding='same', activation=LeakyReLU(alpha=0.2),\n",
    "                            return_sequences=False)(lstm_out1)\n",
    "    \n",
    "    input_one = Lambda(lambda x: x[:, 4, :, :, :])(input1)\n",
    "    input_one = Reshape([128, 128, 1])(input_one)\n",
    "    \n",
    "    cnn_out1   = cnn_model([lstm_out2, input2])\n",
    "    \n",
    "    final_out = Subtract()([input_one, cnn_out1])\n",
    "    #lstm_out1   = ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(5, 128, 128, 1),\n",
    "    #                         padding='same', activation=LeakyReLU(alpha=0.2), return_sequences=True)(lstm_out1)\n",
    "    #lstm_out1   = ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(5, 128, 128, 1),\n",
    "    #                         padding='same', activation=LeakyReLU(alpha=0.2), return_sequences=True)(lstm_out1)\n",
    "    #lstm_out2   = ConvLSTM2D(filters=1, kernel_size=(3, 3), padding='same', activation=LeakyReLU(alpha=0.2),\n",
    "    #                         return_sequences=False)(lstm_out1)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, 4, :, :, 0])(input1)\n",
    "    #input_last  = Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    #conv10      = Subtract()([input1, lstm_out2])\n",
    "    \n",
    "    time_model  = Model(input = [input1, input2], output = final_out)\n",
    "    time_model.compile(optimizer = Adam(lr = 1e-4), \n",
    "                  loss = 'mean_absolute_error', \n",
    "                  metrics = ['mse'])\n",
    "    \n",
    "    return time_model\n",
    "\n",
    "def conv_lstm_model():\n",
    "    seq = Sequential()\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       input_shape=(5, 128, 128, 1),\n",
    "                       padding='same', return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=1, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=False))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "    #               activation='sigmoid',\n",
    "    #               padding='same', data_format='channels_last'))\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def unet_combined(pretrained_weights = None, input_size = (128, 128, 1)):\n",
    "    base_model  = unet()\n",
    "    base_model.load_weights(\"all-data-0-to-7-0.0005-5.h5\")\n",
    "\n",
    "    filter1 = 32\n",
    "    filter2 = 32\n",
    "    filter3 = 32\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input(input_size)\n",
    "    \n",
    "    input1_1 = Input((1, ))\n",
    "    input1_2 = Input((1, ))\n",
    "    input1_3 = Input((1, ))\n",
    "    input1_4 = Input((1, ))\n",
    "    input1_5 = Input((1, ))\n",
    "    input1_6 = Input((1, ))\n",
    "    input1_7 = Input((1, ))\n",
    "    \n",
    "    for t in base_model.layers:\n",
    "        t.trainable = False\n",
    "    \n",
    "    w1    = Concatenate()([input1, input2])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(w1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    \n",
    "    layer_output = base_model.layers[-4].output\n",
    "    \n",
    "    new_model    = Model(base_model.input, layer_output)\n",
    "    \n",
    "    r1 = new_model([input1, input1_1])\n",
    "    r2 = new_model([input1, input1_2])\n",
    "    r3 = new_model([input1, input1_3])\n",
    "    r4 = new_model([input1, input1_4])\n",
    "    r5 = new_model([input1, input1_5])\n",
    "    r6 = new_model([input1, input1_6])\n",
    "    r7 = new_model([input1, input1_7])\n",
    "    \n",
    "#     c1 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 0])(conv1))\n",
    "#     c2 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 1])(conv1))\n",
    "#     c3 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 2])(conv1))\n",
    "#     c4 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 3])(conv1))\n",
    "#     c5 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 4])(conv1))\n",
    "#     c6 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 5])(conv1))\n",
    "#     c7 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 6])(conv1))\n",
    "    \n",
    "    \n",
    "#     q1 = Multiply()([conv1, r1])\n",
    "#     q2 = Multiply()([conv1, r2])\n",
    "#     q3 = Multiply()([conv1, r3])\n",
    "#     q4 = Multiply()([conv1, r4])\n",
    "#     q5 = Multiply()([conv1, r5])\n",
    "#     q6 = Multiply()([conv1, r6])\n",
    "#     q7 = Multiply()([conv1, r7])\n",
    "    \n",
    "    #ut   = Add()([q1, q2, q3, q4, q5, q6, q7])\n",
    "    #ut   = Add()([c1, c2, c3, c4, c5, c6, c7])\n",
    "    out   = Add()([r1, r2, r3, r4, r5, r6, r7])\n",
    "    out   = Multiply()([conv1, out])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    out   = LeakyReLU(0.2)(conv1)\n",
    "    \n",
    "    #conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    model1  = Model(input = [input1, input2, input1_1, input1_2, input1_3, input1_4, input1_5, input1_6, input1_7], output = out)\n",
    "    \n",
    "    model1.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    return model1\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# in1 = np.ones([10, 1, 256, 256])\n",
    "# in2 = np.ones([10, 1])\n",
    "\n",
    "# in1 = torch.tensor(in1, device=device).float()\n",
    "# in2 = torch.tensor(in2, device=device).float()\n",
    "\n",
    "\n",
    "# Define model\n",
    "class MyUnetGN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "        \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU())\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter1//2, filter1))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter2//2, filter2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter3//2, filter3))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter4//2, filter4))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter5//2, filter5))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter4//2, filter4))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter3//2, filter3))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter2//2, filter2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter1//2, filter1))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter4//2, filter4))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter3//2, filter3))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter2//2, filter2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter1//2, filter1))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.1)\n",
    "        self.d2 = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "        \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU())\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter1))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter3))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter4))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter5))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter4))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter3))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter1))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter4))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter3))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2filter1\n",
    "            nn.BatchNorm2d(filter1))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.1)\n",
    "        self.d2 = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice1_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val1)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val1)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val1)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class MyUnetSlice2_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val1)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val1)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val1)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val1 = self.dense_block(y)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val = self.dense_block(v)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val1)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val1)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val1)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block2 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block3 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block4 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block5 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val1 = self.dense_block1(y)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val2 = self.dense_block2(y)\n",
    "        hash_val2 = hash_val2.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val3 = self.dense_block3(y)\n",
    "        hash_val3 = hash_val3.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val4 = self.dense_block4(y)\n",
    "        hash_val4 = hash_val4.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val5 = self.dense_block5(y)\n",
    "        hash_val5 = hash_val5.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val1)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val2)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val3)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val4)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val4)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val3)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val2)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet_double(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 64\n",
    "        filter2 = 128\n",
    "        filter3 = 256\n",
    "        filter4 = 512\n",
    "        filter5 = 1024\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class RatingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            #nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            #nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.out = nn.Sequential(nn.Linear(256, 1),\n",
    "                                 nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        \n",
    "        x = x.view(-1, 256)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "#model = MyUnet()\n",
    "#model.cuda()\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "#model.apply(init_normal)\n",
    "\n",
    "#unet_model = MyUnetGN()\n",
    "#unet_model.cuda()\n",
    "\n",
    "# rating_cnn = RatingModel()\n",
    "# rating_cnn.cuda()\n",
    "\n",
    "#rating_cnn.apply(init_normal)\n",
    "#summary(unet_model, [(1, 256, 256), (1, 1)])\n",
    "\n",
    "# m = rating_cnn()\n",
    "# print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     175
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Getting patches for each lesion for training the calibaration CNN with lesions\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "allfiles = glob.glob(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_annotation/index-b-*.npy\")\n",
    "allindex = []\n",
    "for f in allfiles:\n",
    "    index = int(f.split(\"/\")[-1].split(\"-\")[-1][:-4])\n",
    "    allindex.append(index)\n",
    "\n",
    "patches = {}\n",
    "values  = {}\n",
    "\n",
    "#print(val_list)\n",
    "\n",
    "\n",
    "for img_counter in range(1, 20):\n",
    "    counter = 0\n",
    "    for k in allindex:\n",
    "        #print(h[k][0], h[k], test_list)\n",
    "\n",
    "        if h[k][0] not in test_list:\n",
    "            continue\n",
    "\n",
    "        counter = counter+1\n",
    "        if counter < img_counter:\n",
    "            continue\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        all_outputs = []\n",
    "        all_ssim = []\n",
    "        all_haar = []\n",
    "\n",
    "        patches = []\n",
    "        values  = []\n",
    "\n",
    "        path = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(h[k][0])+\".raw\")[0]\n",
    "        vol  = np.fromfile(path, dtype=\"float32\")\n",
    "        vol  = np.reshape(vol, [64, 1200, 3000])\n",
    "        \n",
    "        px = random.randint(500, 800)\n",
    "        rx = random.randint(1500, 2000)\n",
    "        tx = random.randint(25, 35)\n",
    "        \n",
    "        for tp in range(1, 2):\n",
    "            #tx = 0\n",
    "            #rx = 0\n",
    "            temp = vol[tx, -128+px:128+px, -128+rx:128+rx]\n",
    "            #temp = vol[h[k][3]+tp+tx, h[k][2]-128+rx:h[k][2]+128+rx, h[k][1]-128+rx:h[k][1]+128+rx]\n",
    "            #print(temp.shape)\n",
    "            if temp.shape[0] == 256 and temp.shape[1] == 256:\n",
    "                x.append(temp)\n",
    "                values.append(0)\n",
    "\n",
    "        locations_array = []\n",
    "        locations_flag  = 0\n",
    "\n",
    "        allpaths  = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(h[k][0])+\".raw.npy\")\n",
    "        allpaths.sort()\n",
    "\n",
    "        print(allpaths[0])\n",
    "\n",
    "        for t in allpaths:\n",
    "            #print(t)\n",
    "            s = float(t.split(\"/\")[-1].split(\"-\")[-1].split(\"_\")[0])\n",
    "\n",
    "            vol  = np.load(t)\n",
    "            vol  = np.reshape(vol, [64, 1200, 3000])\n",
    "\n",
    "            temp_count      = 0\n",
    "\n",
    "            for tp in range(1, 2):\n",
    "                #for dx in dx_array:\n",
    "                    #for dy in dy_array:\n",
    "                #rx = 0\n",
    "                #tx = 0\n",
    "                temp = vol[tx, -128+px:128+px, -128+rx:128+rx]\n",
    "                #temp = vol[h[k][3]+tp+tx, h[k][2]-128+rx:h[k][2]+128+rx, h[k][1]-128+rx:h[k][1]+128+rx]\n",
    "                #print(temp.shape)\n",
    "                #temp = vol[h[k][3]+tp, h[k][2]-128+dx:h[k][2]+128+dx, h[k][1]-128+dy:h[k][1]+128+dy]\n",
    "                #print(temp.shape)\n",
    "                if temp.shape[0] == 256 and temp.shape[1] == 256:\n",
    "                    x.append(temp)\n",
    "                    values.append(s)\n",
    "\n",
    "                    #print(s)\n",
    "                    pp1 = np.expand_dims(x[0], 0)\n",
    "                    pp1 = np.expand_dims(pp1,  0)\n",
    "                    pv1 = np.expand_dims(-1*s, 0)\n",
    "                    #print(pp1.shape, pv1.shape)\n",
    "\n",
    "                    pp1 = torch.tensor(pp1, device=device).float()\n",
    "                    pv1 = torch.tensor(pv1, device=device).float()\n",
    "\n",
    "                    result = unet_model.forward(pp1, pv1).data.cpu().numpy()\n",
    "                    all_outputs.append(result)\n",
    "                    #ssim_temp = measure.compare_ssim(result[0, 0, :, :].astype('float32'), temp.astype('float32'), data_range=temp.max() - temp.min())\n",
    "                    t1 = np.min(result[0, 0, :, :].flatten())\n",
    "                    t2 = np.max(result[0, 0, :, :].flatten())\n",
    "                    distorted_image  = (result[0, 0, :, :]-t1)*255/(t2-t1)\n",
    "\n",
    "                    t1 = np.min(temp.flatten())\n",
    "                    t2 = np.max(temp.flatten())\n",
    "                    reference_image = (temp-t1)*255/(t2-t1)\n",
    "\n",
    "                    ssim_temp = measure.compare_ssim(reference_image.astype('float32'), distorted_image.astype('float32'), 255)\n",
    "                    #print(temp.max(), temp.min(), temp.max() - temp.min())\n",
    "\n",
    "                    ssim_temp1 = haar_psi_numpy(reference_image, distorted_image, preprocess_with_subsampling = True)[0]\n",
    "                    all_haar.append(ssim_temp1)\n",
    "                    all_ssim.append(ssim_temp)\n",
    "                    #print(temp.shape, result.shape, ssim_temp, ssim_temp1)\n",
    "\n",
    "        x      = np.array(x)\n",
    "        values = np.array(values)\n",
    "        all_outputs = np.array(all_outputs)\n",
    "        all_outputs = all_outputs[:, 0, 0, :, :]\n",
    "\n",
    "        #print(x.shape, values.shape, all_outputs.shape)\n",
    "        break\n",
    "\n",
    "    #plt.figure(figsize=(40,20))\n",
    "    #plt.axis('off')\n",
    "    tp1 = [x[1], x[3], x[4], x[5], x[6], x[7]]\n",
    "    tv1 = np.array([values[1], values[3], values[4], values[5], values[6], values[7]])\n",
    "    tv1[tv1 > 0.6] = 0.6\n",
    "\n",
    "    ssim_arr = [all_ssim[1-1], all_ssim[3-1], all_ssim[4-1], all_ssim[5-1], all_ssim[6-1], all_ssim[7-1]]\n",
    "    haar_arr = [all_haar[1-1], all_haar[3-1], all_haar[4-1], all_haar[5-1], all_haar[6-1], all_haar[7-1]]\n",
    "\n",
    "    #plt.imshow(np.concatenate(tp1, axis=0).T, cmap='gray')\n",
    "\n",
    "    print(values)\n",
    "    print(tv1)\n",
    "    print(ssim_arr)\n",
    "    print(haar_arr)\n",
    "\n",
    "    tp2 = [all_outputs[1-1], all_outputs[3-1], all_outputs[4-1], all_outputs[5-1], all_outputs[6-1], all_outputs[7-1]]\n",
    "\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = [9.6, 3.4]\n",
    "\n",
    "    #gs1 = gridspec.GridSpec(1, 8)\n",
    "    #gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "    top_title = \"\"\n",
    "    for i in range(1, len(tp2)+1):\n",
    "        if i == 1:\n",
    "            top_title = top_title+r'$\\beta$='+str(np.round(tv1[i-1], 3))\n",
    "        else:\n",
    "            top_title = top_title+r'               $\\beta$='+str(np.round(tv1[i-1], 3))\n",
    "\n",
    "\n",
    "    bottom_title = \"\"\n",
    "    for i in range(1, len(tp2)+1):\n",
    "        if i == 1:\n",
    "            bottom_title = bottom_title+\"SSIM=\"+str(np.round(ssim_arr[i-1], 3))+\",HaarPSI=\"+str(np.round(haar_arr[i-1], 3))\n",
    "        else:\n",
    "            bottom_title = bottom_title+\"   SSIM=\"+str(np.round(ssim_arr[i-1], 3))+\",HaarPSI=\"+str(np.round(haar_arr[i-1], 3))\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        #if i == 1:\n",
    "        axs[0].set_ylabel(\"Ground Truth\")\n",
    "        axs[0].imshow(np.concatenate(tp1).T, cmap='gray')\n",
    "        axs[0].set_title(top_title, y=0.95, fontsize=10)\n",
    "        axs[0].set_xticks([], [])\n",
    "        axs[0].set_yticks([], [])\n",
    "        axs[0].spines['top'].set_visible(False)\n",
    "        axs[0].spines['bottom'].set_visible(False)\n",
    "        axs[0].spines['left'].set_visible(False)\n",
    "        axs[0].spines['right'].set_visible(False)\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        #if i == 1:\n",
    "        axs[1].set_ylabel(\"U-Net Output\")\n",
    "        axs[1].imshow(np.concatenate(tp2).T, cmap='gray')\n",
    "        axs[1].set_title(bottom_title, y=-0.15, fontsize=6)\n",
    "        axs[1].set_xticks([], [])\n",
    "        axs[1].set_yticks([], [])\n",
    "        axs[1].spines['top'].set_visible(False)\n",
    "        axs[1].spines['bottom'].set_visible(False)\n",
    "        axs[1].spines['left'].set_visible(False)\n",
    "        axs[1].spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)\n",
    "    plt.tick_params(\n",
    "        axis='y',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        right=False,\n",
    "        labelbottom=False)\n",
    "    plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')\n",
    "\n",
    "    f.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.savefig('compare_supp_random4_'+str(img_counter)+'.png', dpi=300, bbox_inches = 'tight',\n",
    "        pad_inches = 0.1)\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.show(block=True)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(40,20))\n",
    "# print(values)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(np.concatenate(x[:-2], axis=0).T, cmap='gray')\n",
    "# print(x.shape, values.shape)\n",
    "#np.save(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_cho_data/x_\"+str(k)+\".npy\", x)\n",
    "#np.save(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_cho_data/y_\"+str(k)+\".npy\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] For optimizing the Tuning parameter using the PyTorch Model\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "rating_cnn = RatingModel()\n",
    "rating_cnn.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "rating_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/rating_pytorch.pt')\n",
    "rating_cnn.load_state_dict(rating_weights)\n",
    "\n",
    "\n",
    "#unet_model.eval()\n",
    "#rating_cnn.eval()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device  = torch.device(\"cuda:0\")\n",
    "\n",
    "targets = torch.tensor(np.ones([1, 1]), device=device).float()\n",
    "\n",
    "in1 = x[0, :, :]#np.ones([1, 1, 256, 256]) #np.ones([256, 256])\n",
    "in1 = np.expand_dims(in1, axis=0)\n",
    "in1 = np.expand_dims(in1, axis=1)\n",
    "in1 = torch.tensor(in1, device=device).float()\n",
    "\n",
    "in2 = Variable(torch.tensor(-0.2*np.ones([1, 1], dtype='float32')).cuda(), requires_grad=True)\n",
    "\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.Adam([input_img.requires_grad_()], lr=0.001)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = get_input_optimizer(in2)\n",
    "\n",
    "first_image = 0\n",
    "best_image  = 0\n",
    "best = 1000\n",
    "print('Optimizing..')\n",
    "run = [0]\n",
    "\n",
    "s1 = time.time()\n",
    "while run[0] <= 100:\n",
    "    optimizer.zero_grad()\n",
    "    out1 = unet_model(in1, in2)\n",
    "    out2 = rating_cnn(out1)\n",
    "    \n",
    "    if run[0] == 0:\n",
    "        first_image = out1.data.cpu().numpy()\n",
    "        \n",
    "    loss = criterion(out2, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    run[0] += 1\n",
    "    #if run[0] % 10 == 0:\n",
    "        #print(np.mean(out1.data.cpu().numpy().flatten()))\n",
    "    if loss.item() < best:\n",
    "        best       = loss.item()\n",
    "        best_image = out1.data.cpu().numpy()\n",
    "        #print(\"Loss is \", loss.item(), \" variable \", in2.item(), \" out2 \", out2.item())#, \"out1 \", out1.item())#np.mean(out1.item().flatten()))\n",
    "    optimizer.step()\n",
    "s2 = time.time()\n",
    "print(s2-s1)\n",
    "\n",
    "# Good counters = 11, 12 (16 is best)\n",
    "plt.axis('off')\n",
    "plt.imshow(np.concatenate([x[0, :, :], first_image[0, 0, :, :], best_image[0, 0, :, :]], axis=-1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For filtering the entire slice using PyTorch Model\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "\n",
    "allfiles = glob.glob(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_annotation/index-b-*.npy\")\n",
    "allindex = []\n",
    "for f in allfiles:\n",
    "    index = int(f.split(\"/\")[-1].split(\"-\")[-1][:-4])\n",
    "    allindex.append(index)\n",
    "\n",
    "patches = {}\n",
    "values  = {}\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.Adam([input_img.requires_grad_()], lr=0.001)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "opt_counter = 0\n",
    "for img_counter in range(1, 20):\n",
    "    counter = 0\n",
    "    for k in allindex:\n",
    "        #print(h[k][0], h[k], test_list)\n",
    "\n",
    "        if h[k][0] not in test_list:\n",
    "            continue\n",
    "\n",
    "        counter = counter+1\n",
    "        if counter < img_counter:\n",
    "            continue\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        all_outputs = []\n",
    "        all_ssim    = []\n",
    "        all_haar    = []\n",
    "\n",
    "        patches = []\n",
    "        values  = []\n",
    "\n",
    "        path = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(h[k][0])+\".raw\")[0]\n",
    "        vol  = np.fromfile(path, dtype=\"float32\")\n",
    "        vol  = np.reshape(vol, [64, 1200, 3000])\n",
    "        print(path)\n",
    "        \n",
    "        for tp in range(1, 2):\n",
    "            temp = vol[h[k][3]+tp, :, :]\n",
    "            #print(temp.shape)\n",
    "        \n",
    "        #1280, 3072\n",
    "        image_slice = np.pad(temp, ((40, 40), (36, 36)), 'constant', constant_values=(0, 0))\n",
    "        #print(image_slice.shape)\n",
    "        \n",
    "        temp_all = []\n",
    "        for tk in range(5):\n",
    "            temp_row = []\n",
    "            for tj in range(12):\n",
    "                img = image_slice[tk*256:(tk+1)*256, tj*256:(tj+1)*256]\n",
    "                \n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                t1  = np.expand_dims(img, axis=1)\n",
    "                \n",
    "                t2 = np.expand_dims(-1*np.ones([1, 1])*0.125, axis=-1)\n",
    "                \n",
    "                x1      = torch.tensor(t1, device=device).float()\n",
    "                values1 = torch.tensor(t2, device=device).float()\n",
    "                \n",
    "                output = unet_model.forward(x1, values1)\n",
    "                output = output.data.cpu().numpy()\n",
    "                output = output[0, 0, :, :]\n",
    "                temp_row.append(output)\n",
    "            temp_row = np.concatenate(temp_row, axis=1)\n",
    "            temp_all.append(temp_row)\n",
    "        temp_all1 = np.concatenate(temp_all, axis=0)\n",
    "        \n",
    "        temp_all = []\n",
    "        for tk in range(5):\n",
    "            temp_row = []\n",
    "            for tj in range(12):\n",
    "                img = image_slice[tk*256:(tk+1)*256, tj*256:(tj+1)*256]\n",
    "                \n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                t1  = np.expand_dims(img, axis=1)\n",
    "                \n",
    "                t2 = np.expand_dims(-1*np.ones([1, 1])*0.2, axis=-1)\n",
    "                \n",
    "                x1      = torch.tensor(t1, device=device).float()\n",
    "                values1 = torch.tensor(t2, device=device).float()\n",
    "                \n",
    "                output = unet_model.forward(x1, values1)\n",
    "                output = output.data.cpu().numpy()\n",
    "                output = output[0, 0, :, :]\n",
    "                temp_row.append(output)\n",
    "            temp_row = np.concatenate(temp_row, axis=1)\n",
    "            temp_all.append(temp_row)\n",
    "        temp_all2 = np.concatenate(temp_all, axis=0)\n",
    "        \n",
    "        temp_all2   = temp_all2[40:-40, 36:-36]\n",
    "        temp_all1   = temp_all1[40:-40, 36:-36]\n",
    "        image_slice = image_slice[40:-40, 36:-36]\n",
    "        \n",
    "        image_slice.astype('float32').tofile('image_slice'+str(img_counter)+'_3000x1200.raw')\n",
    "        temp_all1.astype('float32').tofile('result_slice'+str(img_counter)+'_3000x1200.raw')\n",
    "        temp_all2.astype('float32').tofile('result_2slice'+str(img_counter)+'_3000x1200.raw')\n",
    "        \n",
    "        print(temp_all1.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For predicting the result PyTorch Model for Time calculation\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "rating_cnn = RatingModel()\n",
    "rating_cnn.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "rating_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/rating_pytorch.pt')\n",
    "rating_cnn.load_state_dict(rating_weights)\n",
    "\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "#all_outputs.append(x[0, :, :])\n",
    "import time\n",
    "\n",
    "x = np.random.rand(3515, 256, 256)\n",
    "s1 = time.time()\n",
    "for i in range(1, 219):\n",
    "    t1 = np.expand_dims(x[i*16:(i+1)*16, :, :], axis=1)\n",
    "    #1 = [x[i*8:(i+1)*8, :, :]]\n",
    "    t2 = np.expand_dims(-1*np.ones([16, 1])*0.2, axis=-1)\n",
    "    #t2 = np.expand_dims(-1*np.array([values[i]]), axis=-1)\n",
    "    #print(values[i])\n",
    "    \n",
    "    x1      = torch.tensor(t1, device=device).float()\n",
    "    values1 = torch.tensor(t2, device=device).float()\n",
    "    \n",
    "    output = unet_model.forward(x1, values1)\n",
    "    output = output.data.cpu().numpy()\n",
    "    \n",
    "    all_outputs.append(output[0, 0, :, :])\n",
    "all_outputs = np.array(all_outputs)\n",
    "\n",
    "s2 = time.time()\n",
    "\n",
    "print(s2-s1)\n",
    "print(all_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the results\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "o1 = np.concatenate(all_outputs[:7, :, :], axis=0).T\n",
    "print(o1.shape)\n",
    "\n",
    "o2 = np.concatenate(x[1:8, :, :]).T\n",
    "print(o2.shape)\n",
    "\n",
    "#print(o1.shape, o2.shape)\n",
    "\n",
    "together = np.concatenate([o1, o2], axis=0)\n",
    "#print(o1.shape, o2.shape, together.shape)\n",
    "print(values[1:])\n",
    "plt.imshow(together, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     17
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Comparing the Noise Power Spectrum\n",
    "\n",
    "from scipy import fftpack\n",
    "import pyfits\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import radialProfile\n",
    "\n",
    "index  = 2\n",
    "image1 = o1[:, index*256:(index+1)*256]\n",
    "image2 = o2[:, index*256:(index+1)*256]\n",
    "\n",
    "index  = 5\n",
    "image3 = o1[:, index*256:(index+1)*256]\n",
    "image4 = o2[:, index*256:(index+1)*256]\n",
    "\n",
    "\n",
    "def get_power_spectrum(image):    \n",
    "    # Take the fourier transform of the image.\n",
    "    F1 = fftpack.fft2(image)\n",
    "    F2 = fftpack.fftshift(F1)\n",
    "    \n",
    "    # Calculate a 2D power spectrum\n",
    "    psf2D = np.abs( F2 )**2\n",
    "    \n",
    "    # Calculate the azimuthally averaged 1D power spectrum\n",
    "    psf1D = radialProfile.azimuthalAverage(psf2D)\n",
    "    return psf1D\n",
    "\n",
    "t1 = get_power_spectrum(image1)\n",
    "t2 = get_power_spectrum(image2)\n",
    "\n",
    "t3 = get_power_spectrum(image3)\n",
    "t4 = get_power_spectrum(image4)\n",
    "\n",
    "\n",
    "py.figure(3)\n",
    "py.clf()\n",
    "\n",
    "py.semilogy( t1, 'r--', label=r'U-Net             $\\beta = 0.154$')\n",
    "py.semilogy( t3, 'b--', label=r'U-Net             $\\beta = 0.454$')\n",
    "\n",
    "py.semilogy( t2, label=r'Ground Truth $\\beta = 0.154$')\n",
    "py.semilogy( t4, label=r'Ground Truth $\\beta = 0.454$')\n",
    "\n",
    "\n",
    "py.xlabel(\"Spatial Frequency\", fontsize=24)\n",
    "py.ylabel(\"Power Spectrum\",fontsize=24)\n",
    "py.legend(fontsize=20)\n",
    "py.xticks(fontsize=16)\n",
    "py.yticks(fontsize=16)\n",
    "py.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For getting the SSIM metric\n",
    "\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "#from skimage import structural_similarity as ssim\n",
    "from skimage import measure\n",
    "import glob\n",
    "\n",
    "import importlib\n",
    "importlib.reload(haarPsi)\n",
    "\n",
    "#import \n",
    "from haarPsi import haar_psi_numpy\n",
    "\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "ssim_array = {}\n",
    "ssim_array[0.1] = []\n",
    "ssim_array[0.2] = []\n",
    "ssim_array[0.3] = []\n",
    "ssim_array[0.4] = []\n",
    "ssim_array[0.5] = []\n",
    "ssim_array[0.6] = []\n",
    "\n",
    "\n",
    "mae_array = {}\n",
    "mae_array[0.1] = []\n",
    "mae_array[0.2] = []\n",
    "mae_array[0.3] = []\n",
    "mae_array[0.4] = []\n",
    "mae_array[0.5] = []\n",
    "mae_array[0.6] = []\n",
    "\n",
    "def my_mae(x, y):\n",
    "    return np.mean(np.abs(x-y))\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "checkit = []\n",
    "\n",
    "flag = False\n",
    "\n",
    "for t in test_list:\n",
    "    a        = np.load(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/PATCHES/\"+str(t)+\"_0.npy\")\n",
    "    allfiles = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/PATCHES/\"+str(t)+\"_-*.npy\")\n",
    "    \n",
    "    for f in allfiles:\n",
    "        value = -1*float(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "        b     = np.load(f)\n",
    "        print(f, value)\n",
    "        \n",
    "        for index in range(50):\n",
    "            b1    = b[index, :, :, 0]\n",
    "            \n",
    "            t1    = a[index, :, :, 0]\n",
    "            t1    = np.expand_dims(t1, 0)\n",
    "            t1    = np.expand_dims(t1, 1)\n",
    "            \n",
    "            values  = -1*value*np.ones([1, 1])\n",
    "            \n",
    "            x1      = torch.tensor(t1, device=device).float()\n",
    "            values  = torch.tensor(values, device=device).float()\n",
    "        \n",
    "            output = unet_model.forward(x1, values)\n",
    "            pred   = output.data.cpu().numpy()\n",
    "            \n",
    "            t1 = np.min(b1.flatten())\n",
    "            t2 = np.max(b1.flatten())\n",
    "            reference_image = (b1-t1)*255/(t2-t1)\n",
    "            \n",
    "            t1 = np.min(pred[0, 0, :, :].flatten())\n",
    "            t2 = np.max(pred[0, 0, :, :].flatten())\n",
    "            distorted_image = (pred[0, 0, :, :]-t1)*255/(t2-t1)\n",
    "            #print(np.min(distorted_image), np.max(distorted_image), np.max(reference_image), np.min(reference_image))\n",
    "            \n",
    "            ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "            #ssim_temp = measure.compare_ssim(pred[0, 0, :, :].astype('float16'), b1.astype('float16'))\n",
    "            #ssim_temp = measure.compare_ssim(pred[0, 0, :, :].astype('float32'), b1.astype('float32'), data_range=b1.max() - b1.min())\n",
    "            #ssim_temp = measure.compare_ssim(pred[0, 0, :, :].astype('float32'), b1.astype('float32'), data_range=pred[0, 0, :, :].max() - pred[0, 0, :, :].min())\n",
    "            \n",
    "            #ssim_temp = haar_psi_numpy(reference_image, distorted_image, preprocess_with_subsampling = True)[0]\n",
    "            if ssim_temp < 0.6 and value < 0.6:\n",
    "                import random\n",
    "                lp = random.randint(0, 100)\n",
    "                if lp > 90:\n",
    "                    flag       = True\n",
    "                    ssim_temp1 = haar_psi_numpy(reference_image.astype('float32'), distorted_image.astype('float32'),  preprocess_with_subsampling = True)[0]\n",
    "                    print(ssim_temp, value, ssim_temp1)\n",
    "                    checkit.append([reference_image, distorted_image])\n",
    "                    break\n",
    "            #print(ssim_temp[0], ssim_temp[1].shape)\n",
    "            \n",
    "            mae_temp  = my_mae(pred[0, 0, :, :], b1)\n",
    "                \n",
    "            if value < 0.1:\n",
    "                ssim_array[0.1].append(ssim_temp)\n",
    "                mae_array[0.1].append(mae_temp)\n",
    "            elif value < 0.2:\n",
    "                ssim_array[0.2].append(ssim_temp)\n",
    "                mae_array[0.2].append(mae_temp)    \n",
    "            elif  value < 0.3:\n",
    "                ssim_array[0.3].append(ssim_temp)\n",
    "                mae_array[0.3].append(mae_temp)    \n",
    "            elif  value < 0.4:\n",
    "                ssim_array[0.4].append(ssim_temp)\n",
    "                mae_array[0.4].append(mae_temp)    \n",
    "            elif value < 0.5:\n",
    "                ssim_array[0.5].append(ssim_temp)\n",
    "                mae_array[0.5].append(mae_temp)    \n",
    "            elif value < 0.6:\n",
    "                ssim_array[0.6].append(ssim_temp)\n",
    "                mae_array[0.6].append(mae_temp)\n",
    "        if flag:\n",
    "            break\n",
    "    if flag:\n",
    "        break\n",
    "\n",
    "values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "for t in values:\n",
    "    print(t, np.mean(ssim_array[t]), np.mean(mae_array[t]), np.std(ssim_array[t]), np.std(mae_array[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting Power Spectrum\n",
    "\n",
    "from scipy import fftpack\n",
    "import pyfits\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import radialProfile\n",
    "\n",
    "image = #pyfits.getdata(myimage.fits)\n",
    "\n",
    "# Take the fourier transform of the image.\n",
    "F1 = fftpack.fft2(image)\n",
    "F2 = fftpack.fftshift( F1 )from scipy import fftpack\n",
    "import pyfits\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import radialProfile\n",
    "\n",
    "image = #pyfits.getdata(myimage.fits)\n",
    "\n",
    "# Take the fourier transform of the image.\n",
    "F1 = fftpack.fft2(image)\n",
    "F2 = fftpack.fftshift( F1 )\n",
    "\n",
    "# Calculate a 2D power spectrum\n",
    "psd2D = np.abs( F2 )**2\n",
    "\n",
    "# Calculate the azimuthally averaged 1D power spectrum\n",
    "psd1D = radialProfile.azimuthalAverage(psd2D)\n",
    "\n",
    "# Now plot up both\n",
    "py.figure(1)\n",
    "py.clf()\n",
    "py.imshow( np.log10( image ), cmap=py.cm.Greys)\n",
    "\n",
    "py.figure(2)\n",
    "py.clf()\n",
    "py.imshow( np.log10( psf2D ))\n",
    "\n",
    "py.figure(3)\n",
    "py.clf()\n",
    "py.semilogy( psf1D )\n",
    "py.xlabel(Spatial Frequency)\n",
    "py.ylabel(Power Spectrum)\n",
    "\n",
    "py.show()\n",
    "\n",
    "# Calculate a 2D power spectrum\n",
    "psd2D = np.abs( F2 )**2\n",
    "\n",
    "# Calculate the azimuthally averaged 1D power spectrum\n",
    "psd1D = radialProfile.azimuthalAverage(psd2D)\n",
    "\n",
    "# Now plot up both\n",
    "py.figure(1)\n",
    "py.clf()\n",
    "py.imshow( np.log10( image ), cmap=py.cm.Greys)\n",
    "\n",
    "py.figure(2)\n",
    "py.clf()\n",
    "py.imshow( np.log10( psf2D ))\n",
    "\n",
    "py.figure(3)\n",
    "py.clf()\n",
    "py.semilogy( psf1D )\n",
    "py.xlabel(Spatial Frequency)\n",
    "py.ylabel(Power Spectrum)\n",
    "\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     29
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Rating CNN in Pytorch\n",
    "\n",
    "trainx = np.zeros([160, 256, 256, 1])\n",
    "trainy = np.zeros([160, 1])\n",
    "valx   = np.zeros([230, 256, 256, 1])\n",
    "valy   = np.zeros([230, 1])\n",
    "\n",
    "train_list = [1, 3, 6, 7, 11, 12, 13, 16, 17, 31, 33, 35, 37, 39, 41, 43, 50, 52, 55,  68, 69, 70, 71, 72, 73, 76]\n",
    "val_list   = [19, 21, 23, 25, 27, 29, 61, 62, 64, 65]\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "traincount = 0\n",
    "valcount   = 0\n",
    "\n",
    "allfiles = glob.glob(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_ratings/x_*.npy\")\n",
    "for f in allfiles:\n",
    "    #print(f)\n",
    "    \n",
    "    x1 = np.load(f)\n",
    "    y1 = -1*np.load(f.replace(\"x_\", \"z_\"))\n",
    "    \n",
    "    k = int(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "    #print(k, h[k])\n",
    "    \n",
    "    x1 = np.expand_dims(x1, axis=-1)\n",
    "    y1 = np.expand_dims(y1, axis=-1)\n",
    "    \n",
    "    #print(x1.shape, y1.shape)\n",
    "    \n",
    "    if h[k][0] in train_list:\n",
    "        #print(x1.shape, y1.shape, \"Train\")\n",
    "        if x1.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        trainx[traincount:traincount+x1.shape[0], :, :, :] = x1\n",
    "        trainy[traincount:traincount+x1.shape[0], :] = y1\n",
    "        #trainx.append(x1)\n",
    "        #trainy.append(y1)\n",
    "        traincount = traincount+x1.shape[0]\n",
    "    elif h[k][0] in test_list:\n",
    "        print(f)\n",
    "        #print(x1.shape, y1.shape, \"Val\")\n",
    "        if x1.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        #valx[valcount:valcount+x1.shape[0], :, :, :] = x1\n",
    "        #valy[valcount:valcount+x1.shape[0], :] = y1\n",
    "        #valx.append(x1)\n",
    "        #valy.append(y1)\n",
    "        valcount = valcount+x1.shape[0]\n",
    "\n",
    "print(traincount, valcount)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Code to get the patches\n",
    "import numpy as np\n",
    "\n",
    "#todo = [15, 16, 19, 20, 27, 29, 31, 35]\n",
    "\n",
    "train_list = [1, 3, 6, 7, 11, 12, 13, 16, 17, 31, 33, 35, 37, 39, 41, 43, 50, 52, 55,  68, 69, 70, 71, 72, 73, 76]\n",
    "val_list   = [19, 21, 23, 25, 27, 29, 61, 62, 64, 65]\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "all_list = np.concatenate([train_list, val_list, test_list])\n",
    "\n",
    "for i in range(1, 77):\n",
    "    if i not in all_list:\n",
    "        continue\n",
    "    \n",
    "    #todo = [15, 16, 19, 20, 27, 29, 31, 35]\n",
    "    #allfiles = glob.glob(\"/media/drilnvm/ubuntudata2/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(i)+\".raw\")\n",
    "    allfiles = glob.glob(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/RECONS-NORMAL-QUADRATIC/*_\"+str(i)+\".raw\")\n",
    "    allfiles.sort(key=lambda x: float(x.split('/')[-1].split('_')[-2]), reverse=False)\n",
    "    \n",
    "    mainfile = allfiles[0]#glob.glob(\"/media/drilnvm/ubuntudata2/REAL-DBT-PROJECTIONS/RECONS-LINEAR/*_\"+str(i)+\".raw\")[0]\n",
    "    \n",
    "    all_vols   = np.zeros([len(allfiles), 40, 1200, 3000], dtype='float16')\n",
    "    all_index  = []\n",
    "    all_values = []\n",
    "    all_values_z = []\n",
    "    \n",
    "    #temp = np.fromfile(mainfile, dtype='float32')\n",
    "    #temp = np.reshape(temp, [40, 1200, 3000])\n",
    "    #all_vols[0, :, :, :] = temp\n",
    "    #all_index.append(i)\n",
    "    #all_values.append(-0)\n",
    "    \n",
    "    counter = 0\n",
    "    for f in allfiles:\n",
    "        a = np.fromfile(f, dtype='float32')\n",
    "        a = np.reshape(a, [40, 1200, 3000]).astype('float16')\n",
    "        #a     = np.load(f)\n",
    "        #a     = np.reshape(a, [40, 1200, 3000])\n",
    "        all_vols[counter, :, :, :] = a\n",
    "        \n",
    "        index = int(f.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "        value = float(f.split(\"/\")[-1].split(\"_\")[-2])\n",
    "        \n",
    "        all_index.append(index)\n",
    "        all_values.append(value)\n",
    "        #all_values_z.append()\n",
    "        \n",
    "        print(index, value, a.shape, f)\n",
    "        counter = counter+1\n",
    "    \n",
    "    total_count   = 50\n",
    "    all_locations = []\n",
    "    \n",
    "    print('Main file: ', mainfile)\n",
    "    \n",
    "    # Get all Locations\n",
    "    while(True):\n",
    "        ix = np.random.randint(256, 1200-256)\n",
    "        iy = np.random.randint(256, 3000-256)\n",
    "        iz = np.random.randint(5, 35)\n",
    "        \n",
    "        tempx = all_vols[0][iz, ix:ix+256, iy:iy+256]\n",
    "        \n",
    "        if np.count_nonzero(tempx.flatten())*1.0/(256*256) < 0.9:\n",
    "            continue\n",
    "        \n",
    "        all_locations.append([ix, iy, iz])\n",
    "        if len(all_locations) == total_count:\n",
    "            break\n",
    "    \n",
    "    # Get patches\n",
    "    for k in range(len(all_vols)):\n",
    "        y_array = np.zeros([total_count, 256, 256, 1], dtype='float16')\n",
    "        counter = 0\n",
    "        \n",
    "        all_values_z = []\n",
    "        for p in all_locations:\n",
    "            iz = p[2]\n",
    "            ix = p[0]\n",
    "            iy = p[1]\n",
    "            \n",
    "            all_values_z.append(iz)\n",
    "            tempy   = all_vols[k][iz, ix:ix+256, iy:iy+256]\n",
    "            y_array[counter, :, :, 0] = tempy\n",
    "            counter = counter+1\n",
    "        np.save(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(all_index[k])+'_'+str(all_values[k])+'.npy', y_array)\n",
    "        np.save(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/zvalue-\"+str(all_index[k])+'_'+str(all_values[k])+'.npy',   all_values_z)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For reading the training data\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "#train_list = [1, 3, 6, 7, 11, 12, 13, 16, 17, 31, 33, 35, 37, 39, 41, 43, 50, 52, 55,  68, 69, 70, 71, 72, 73, 76]\n",
    "train_list = [1, 3, 6, 7, 11, 12, 13, 16]#, 17, 31, 33, 35, 37, 39]\n",
    "val_list   = [19, 21, 23, 25, 27, 29, 61, 62, 64, 65]\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "# 10650, 3950\n",
    "# 8200, 3160\n",
    "# 10250, 3950\n",
    "\n",
    "#11700\n",
    "#10400\n",
    "train_size = 3200\n",
    "x_array = np.zeros([train_size, 1, 256, 256], dtype='float16')\n",
    "z_array = np.zeros([train_size, 1], dtype='float16')\n",
    "v_array = np.zeros([train_size, 1], dtype='float16')\n",
    "y_array = np.zeros([train_size, 1, 256, 256], dtype='float16')\n",
    "\n",
    "# 4500\n",
    "# 3950\n",
    "val_size = 4000\n",
    "x_val_array = np.zeros([val_size, 1, 256, 256], dtype='float16')\n",
    "z_val_array = np.zeros([val_size, 1], dtype='float16')\n",
    "v_val_array = np.zeros([val_size, 1], dtype='float16')\n",
    "y_val_array = np.zeros([val_size, 1, 256, 256], dtype='float16')\n",
    "\n",
    "count = 0\n",
    "for t in train_list:\n",
    "    #print(t)\n",
    "    maintemp = np.load(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_-0.0.npy\")\n",
    "    #maintemp = np.load(mainfile)\n",
    "    \n",
    "    allfiles = glob.glob(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_*.npy\")\n",
    "    for f in allfiles:\n",
    "        if '-0.0.npy' in f:\n",
    "            continue\n",
    "        temp  = np.load(f)\n",
    "        temp1 = np.load(f.replace('imgarray', 'zvalue')) \n",
    "        #print(temp.shape)\n",
    "        print(f, temp.shape, float(f.split(\"/\")[-1].split(\"_\")[1][:-4]))\n",
    "        y_array[count: count+50, 0, :, :] = temp[:, :, :, 0]\n",
    "        x_array[count: count+50, 0, :, :] = maintemp[:, :, :, 0]\n",
    "        z_array[count: count+50, :]       = -np.ones([50, 1])*float(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "        v_array[count: count+50, :]       = np.expand_dims(temp1, axis=1)\n",
    "        count = count+50\n",
    "\n",
    "print(x_array.shape, y_array.shape, z_array.shape, count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for t in val_list:\n",
    "    #print(t)\n",
    "    maintemp = np.load(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_-0.0.npy\")\n",
    "    #maintemp = np.load(mainfile)\n",
    "    \n",
    "    allfiles = glob.glob(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_*.npy\")\n",
    "    for f in allfiles:\n",
    "        if '-0.0.npy' in f:\n",
    "            continue\n",
    "        temp  = np.load(f)\n",
    "        temp1 = np.load(f.replace('imgarray', 'zvalue'))\n",
    "        \n",
    "        print(f, temp.shape, float(f.split(\"/\")[-1].split(\"_\")[1][:-4]))\n",
    "        y_val_array[count: count+50, 0, :, :] = temp[:50, :, :, 0]\n",
    "        x_val_array[count: count+50, 0, :, :] = maintemp[:50, :, :, 0]\n",
    "        z_val_array[count: count+50, :]       = -np.ones([50, 1])*float(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "        v_val_array[count: count+50, :]           = np.expand_dims(temp1, axis=1)\n",
    "        count = count+50\n",
    "        \n",
    "print(x_val_array.shape, y_val_array.shape, z_val_array.shape, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Rating CNN pytorch\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(rating_cnn.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min = 1000\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    rating_cnn.train()\n",
    "    loss_array = []\n",
    "    \n",
    "    for i in range(len(trainx)//32):\n",
    "        x = trainx[i*32:(i+1)*32, :, :, :]\n",
    "        y = trainy[i*32:(i+1)*32, :]\n",
    "        #z = z_array[i*8:(i+1)*8, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        #z = torch.tensor(z, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = rating_cnn(x)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape, y.data.shape)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i %100 == 0:\n",
    "            rating_cnn.eval()\n",
    "            loss_array_val = []\n",
    "            for ik in range(len(valx)//8):\n",
    "                x = valx[ik*8:(ik+1)*8, :, :, :]\n",
    "                y = valy[ik*8:(ik+1)*8, :]\n",
    "                #z = z_val_array[i*8:(i+1)*8, :]\n",
    "\n",
    "                x = torch.tensor(x, device=device).float()\n",
    "                y = torch.tensor(y, device=device).float()\n",
    "                #z = torch.tensor(z, device=device).float()\n",
    "\n",
    "                output = rating_cnn(x)\n",
    "\n",
    "                loss = criterion(output, y)\n",
    "                loss_array_val.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(loss_array_val)\n",
    "            print(\"Val loss \", val_loss)\n",
    "\n",
    "            if val_loss < prev_min:\n",
    "                prev_min = val_loss\n",
    "                torch.save(rating_cnn.state_dict(), \"rating_pytorch.pt\")\n",
    "            rating_cnn.train()\n",
    "    print(np.mean(loss_array))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     22
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result with volume Slice Huber\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.1] = []\n",
    "    ssim_array[0.2] = []\n",
    "    ssim_array[0.3] = []\n",
    "    ssim_array[0.4] = []\n",
    "    ssim_array[0.5] = []\n",
    "    ssim_array[0.6] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        \n",
    "        if value < 0.1:\n",
    "            ssim_array[0.1].append(ssim_temp)\n",
    "            #mae_array[0.1].append(mae_temp)\n",
    "        elif value < 0.2:\n",
    "            ssim_array[0.2].append(ssim_temp)\n",
    "            #mae_array[0.2].append(mae_temp)    \n",
    "        elif  value < 0.3:\n",
    "            ssim_array[0.3].append(ssim_temp)\n",
    "            #mae_array[0.3].append(mae_temp)    \n",
    "        elif  value < 0.4:\n",
    "            ssim_array[0.4].append(ssim_temp)\n",
    "            #mae_array[0.4].append(mae_temp)    \n",
    "        elif value < 0.5:\n",
    "            ssim_array[0.5].append(ssim_temp)\n",
    "            #mae_array[0.5].append(mae_temp)    \n",
    "        elif value < 0.6:\n",
    "            ssim_array[0.6].append(ssim_temp)\n",
    "            #mae_array[0.6].append(mae_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 3)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()#_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 8\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    \n",
    "    ssim_z_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    z_array = z_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss1  = my_loss(output, y)\n",
    "        #loss2  = mse_criterion(output, y)\n",
    "        \n",
    "        loss   = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    \n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss1 = my_loss(output, y)\n",
    "        #loss2 = mse_criterion(output, y)\n",
    "        loss  = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "        \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss, ' SSIM ', ssim_string)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-quadratic.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     22,
     154
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result with volume Slice Quadratic Linear\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.025] = []\n",
    "    ssim_array[0.050] = []\n",
    "    ssim_array[0.075] = []\n",
    "    ssim_array[0.100] = []\n",
    "    ssim_array[0.125] = []\n",
    "    ssim_array[0.150] = []\n",
    "    ssim_array[0.175] = []\n",
    "    ssim_array[0.200] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        \n",
    "        #print('value is ', value)\n",
    "        if np.isclose(-value, 0.025, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.025].append(ssim_temp)\n",
    "            #mae_array[0.1].append(mae_temp)\n",
    "        elif np.isclose(-value, 0.050, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.050].append(ssim_temp)\n",
    "            #mae_array[0.2].append(mae_temp)    \n",
    "        elif  np.isclose(-value, 0.075, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.075].append(ssim_temp)\n",
    "            #mae_array[0.3].append(mae_temp)    \n",
    "        elif  np.isclose(-value, 0.100, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.100].append(ssim_temp)\n",
    "            #mae_array[0.4].append(mae_temp)    \n",
    "        elif np.isclose(-value, 0.125, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.125].append(ssim_temp)\n",
    "            #mae_array[0.5].append(mae_temp)    \n",
    "        elif np.isclose(-value, 0.150, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.150].append(ssim_temp)\n",
    "        elif np.isclose(-value, 0.175, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.175].append(ssim_temp)\n",
    "        else:\n",
    "            ssim_array[0.200].append(ssim_temp)\n",
    "            #mae_array[0.6].append(mae_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 3)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()#_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 8\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    \n",
    "    ssim_z_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    z_array = z_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss1  = my_loss(output, y)\n",
    "        #loss2  = mse_criterion(output, y)\n",
    "        \n",
    "        loss   = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    \n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss1 = my_loss(output, y)\n",
    "        #loss2 = mse_criterion(output, y)\n",
    "        loss  = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "        \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss, ' SSIM ', ssim_string)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-quadratic-linear.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     22,
     99,
     154
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result with volume Slice Quadratic Normal\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.025] = []\n",
    "    ssim_array[0.050] = []\n",
    "    ssim_array[0.075] = []\n",
    "    ssim_array[0.100] = []\n",
    "    ssim_array[0.125] = []\n",
    "    ssim_array[0.150] = []\n",
    "    ssim_array[0.175] = []\n",
    "    ssim_array[0.200] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        \n",
    "        #print('value is ', value)\n",
    "        if -value < 0.025:\n",
    "            ssim_array[0.025].append(ssim_temp)\n",
    "            #mae_array[0.1].append(mae_temp)\n",
    "        elif -value < 0.050:\n",
    "            ssim_array[0.050].append(ssim_temp)\n",
    "            #mae_array[0.2].append(mae_temp)    \n",
    "        elif -value < 0.075:\n",
    "            ssim_array[0.075].append(ssim_temp)\n",
    "            #mae_array[0.3].append(mae_temp)    \n",
    "        elif  -value < 0.100:\n",
    "            ssim_array[0.100].append(ssim_temp)\n",
    "            #mae_array[0.4].append(mae_temp)    \n",
    "        elif -value < 0.125:\n",
    "            ssim_array[0.125].append(ssim_temp)\n",
    "            #mae_array[0.5].append(mae_temp)    \n",
    "        elif -value < 0.150:\n",
    "            ssim_array[0.150].append(ssim_temp)\n",
    "        elif -value < 0.175:\n",
    "            ssim_array[0.175].append(ssim_temp)\n",
    "        else:\n",
    "            ssim_array[0.200].append(ssim_temp)\n",
    "            #mae_array[0.6].append(mae_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 3)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()#_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 8\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    \n",
    "    ssim_z_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    z_array = z_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss1  = my_loss(output, y)\n",
    "        #loss2  = mse_criterion(output, y)\n",
    "        \n",
    "        loss   = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    \n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss1 = my_loss(output, y)\n",
    "        #loss2 = mse_criterion(output, y)\n",
    "        loss  = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "        \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss, ' SSIM ', ssim_string)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-quadratic-normal-quarter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     56
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "model = MyUnet()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 4\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss = my_loss(output, y)\n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(np.mean(loss_array))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss = my_loss(output, y)\n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unet_pytorch_new-3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011110814263176500001700-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011110814263176500000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011110814263176500001369-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011110814263176500001056-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04168416 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071114193343700001676-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071114193343700001050-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071114193343700000737-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071114193343700001384-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04042903 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011100313335364000001600-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011100313335364000002121-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011100313335364000001329-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011100313335364000001853-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04153085 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022814504125000002079-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022814504125000002389-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022814504125000001486-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022814504125000001799-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04117774 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000012081714484135900007119-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012081714484135900006830-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012081714484135900006526-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012081714484135900007426-L-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04165057 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013061014482539000001622-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013061014482539000001023-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013061014482539000000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013061014482539000001300-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04143299 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013011818463067100000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013011818463067100001053-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013011818463067100001643-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013011818463067100001330-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04040554 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011092714151459300000722-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011092714151459300001038-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011092714151459300001664-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011092714151459300001339-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04169421 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071218123906200001047-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071218123906200001348-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071218123906200000737-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013071218123906200001649-L-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04155798 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013043018362012500001734-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013043018362012500000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013043018362012500001208-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013043018362012500002134-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04146720 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013012815305089000002233-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013012815305089000002486-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013012815305089000001730-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013012815305089000001983-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04150918 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013021116040581200000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013021116040581200001354-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013021116040581200001038-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013021116040581200001685-L-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04033873 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011081814590068700001934-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011081814590068700001353-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011081814590068700001621-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011081814590068700002226-L-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04152269 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022113174948400001359-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022113174948400001693-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022113174948400001055-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013022113174948400000736-R-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04167351 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013062513081828100001864-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013062513081828100001476-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013062513081828100001157-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013062513081828100000844-R-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04160509 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051716133384300002230-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051716133384300001495-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051716133384300001995-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051716133384300001748-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04043024 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011101021430951500000904-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011101021430951500000362-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011101021430951500000115-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011101021430951500000627-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04164418 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500001885-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500002592-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500002264-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500001542-R-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04154843 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013032616033221800001874-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013032616033221800001089-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013032616033221800000731-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013032616033221800001456-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04079872 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000004005-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000003397-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000004339-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000003701-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04032431 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011080413494359300007008-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011080413494359300007339-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011080413494359300008034-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011080413494359300007682-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04162609 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052415382250000002397-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052415382250000002108-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052415382250000001822-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052415382250000001530-R-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04086303 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800003612-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800004142-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800004407-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800003880-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04141194 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000012110218264857800000978-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012110218264857800001499-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012110218264857800000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012110218264857800001234-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04081295 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000005749-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000006282-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000006014-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011111414283725000006559-L-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04163239 104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500000547-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500000228-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500001173-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013052420144937500000863-R-MLO 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04159728 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051312300895300001601-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051312300895300000734-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051312300895300001306-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000013051312300895300001029-L-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000012103115070775000001679-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012103115070775000001327-R-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012103115070775000001035-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000012103115070775000000734-R-CC 26\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04085413 104\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800002775-R-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800002986-L-CC 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800003402-L-MLO 26\n",
      "1.3.12.2.1107.5.12.7.1446.30000011112314350521800003203-R-MLO 26\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Code to move the files into respective folders\n",
    "\n",
    "import os\n",
    "\n",
    "for folder in glob.glob('/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/*')[2:]:\n",
    "    allfiles = glob.glob(folder+'/*IMA')\n",
    "    print(folder, len(allfiles))\n",
    "    \n",
    "    h        = {}\n",
    "    for f in allfiles:\n",
    "        a      = pydicom.dcmread(f)\n",
    "        name   = a.SeriesInstanceUID+'-'+a.ImageLaterality+'-'+a.ViewPosition\n",
    "        if name in h: \n",
    "            h[name].append(f)\n",
    "        else:\n",
    "            h[name] = []\n",
    "            h[name].append(f)\n",
    "        \n",
    "    for k in h:\n",
    "        print(k, len(h[k]))\n",
    "        new_folder = folder+'/LE-'+k.split('-')[1]+'-'+k.split('-')[2]\n",
    "        os.mkdir(new_folder)\n",
    "        \n",
    "        for p in h[k]:\n",
    "            os.rename(p, new_folder+'/'+p.split('/')[-1])\n",
    "            #print(new_folder+'/'+p.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04168416\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04168416/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04168416/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04168416/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04168416/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04042903\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04042903/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04042903/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04042903/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04042903/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04153085\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04153085/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04153085/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04153085/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04153085/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04117774\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04117774/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04117774/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04117774/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04117774/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04165057\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04165057/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04165057/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04165057/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04165057/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04143299\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04143299/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04143299/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04143299/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04143299/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04040554\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04040554/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04040554/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04040554/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04040554/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04169421\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04169421/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04169421/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04169421/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04169421/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04155798\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04155798/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04155798/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04155798/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04155798/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04146720\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04146720/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04146720/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04146720/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04146720/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04150918\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04150918/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04150918/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04150918/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04150918/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04033873\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04033873/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04033873/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04033873/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04033873/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04152269\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04152269/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04152269/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04152269/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04152269/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04167351\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04167351/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04167351/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04167351/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04167351/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04160509\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04160509/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04160509/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04160509/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04160509/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04043024\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04043024/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04043024/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04043024/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04043024/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04164418\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04164418/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04164418/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04164418/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04164418/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04154843\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04154843/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04154843/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04154843/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04154843/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04079872\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04079872/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04079872/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04079872/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04079872/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04032431\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04032431/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04032431/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04032431/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04032431/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04162609\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04162609/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04162609/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04162609/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04162609/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04086303\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04086303/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04086303/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04086303/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04086303/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04141194\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04141194/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04141194/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04141194/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04141194/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04081295\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04081295/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04081295/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04081295/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04081295/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04163239\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04163239/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04163239/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04163239/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04163239/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04159728\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04159728/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04159728/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04159728/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04159728/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608/LE-R-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04085413\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04085413/LE-L-MLO/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04085413/LE-L-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04085413/LE-R-CC/\n",
      "/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04085413/LE-R-MLO/\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Code to move the files into respective folders\n",
    "\n",
    "import os\n",
    "\n",
    "count = 107\n",
    "\n",
    "for folder in glob.glob('/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/*')[2:]:\n",
    "    allfiles = glob.glob(folder+'/*')\n",
    "    print(folder)# len(allfiles))\n",
    "    for t in allfiles:\n",
    "        temp = t+'/'\n",
    "        print(temp)\n",
    "        np.save('/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/ALL-SETTINGS/settings-'+str(count)+'.npy', temp)\n",
    "        count = count+1\n",
    "#     h        = {}\n",
    "#     for f in allfiles:\n",
    "#         a      = pydicom.dcmread(f)\n",
    "#         name   = a.SeriesInstanceUID+'-'+a.ImageLaterality+'-'+a.ViewPosition\n",
    "#         if name in h: \n",
    "#             h[name].append(f)\n",
    "#         else:\n",
    "#             h[name] = []\n",
    "#             h[name].append(f)\n",
    "        \n",
    "#     for k in h:\n",
    "#         print(k, len(h[k]))\n",
    "#         new_folder = folder+'/LE-'+k.split('-')[1]+'-'+k.split('-')[2]\n",
    "#         os.mkdir(new_folder)\n",
    "        \n",
    "#         for p in h[k]:\n",
    "#             os.rename(p, new_folder+'/'+p.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python sart-all-real.py 107\n",
      "python sart-all-real.py 108\n",
      "python sart-all-real.py 109\n",
      "python sart-all-real.py 110\n",
      "python sart-all-real.py 111\n",
      "python sart-all-real.py 112\n",
      "python sart-all-real.py 113\n",
      "python sart-all-real.py 114\n",
      "python sart-all-real.py 115\n",
      "python sart-all-real.py 116\n",
      "python sart-all-real.py 117\n",
      "python sart-all-real.py 118\n",
      "python sart-all-real.py 119\n",
      "python sart-all-real.py 120\n",
      "python sart-all-real.py 121\n",
      "python sart-all-real.py 122\n",
      "python sart-all-real.py 123\n",
      "python sart-all-real.py 124\n",
      "python sart-all-real.py 125\n",
      "python sart-all-real.py 126\n",
      "python sart-all-real.py 127\n",
      "python sart-all-real.py 128\n",
      "python sart-all-real.py 129\n",
      "python sart-all-real.py 130\n",
      "python sart-all-real.py 131\n",
      "python sart-all-real.py 132\n",
      "python sart-all-real.py 133\n",
      "python sart-all-real.py 134\n",
      "python sart-all-real.py 135\n",
      "python sart-all-real.py 136\n",
      "python sart-all-real.py 137\n",
      "python sart-all-real.py 138\n",
      "python sart-all-real.py 139\n",
      "python sart-all-real.py 140\n",
      "python sart-all-real.py 141\n",
      "python sart-all-real.py 142\n",
      "python sart-all-real.py 143\n",
      "python sart-all-real.py 144\n",
      "python sart-all-real.py 145\n",
      "python sart-all-real.py 146\n",
      "python sart-all-real.py 147\n",
      "python sart-all-real.py 148\n",
      "python sart-all-real.py 149\n",
      "python sart-all-real.py 150\n",
      "python sart-all-real.py 151\n",
      "python sart-all-real.py 152\n",
      "python sart-all-real.py 153\n",
      "python sart-all-real.py 154\n",
      "python sart-all-real.py 155\n",
      "python sart-all-real.py 156\n",
      "python sart-all-real.py 157\n",
      "python sart-all-real.py 158\n",
      "python sart-all-real.py 159\n",
      "python sart-all-real.py 160\n",
      "python sart-all-real.py 161\n",
      "python sart-all-real.py 162\n",
      "python sart-all-real.py 163\n",
      "python sart-all-real.py 164\n",
      "python sart-all-real.py 165\n",
      "python sart-all-real.py 166\n",
      "python sart-all-real.py 167\n",
      "python sart-all-real.py 168\n",
      "python sart-all-real.py 169\n",
      "python sart-all-real.py 170\n",
      "python sart-all-real.py 171\n",
      "python sart-all-real.py 172\n",
      "python sart-all-real.py 173\n",
      "python sart-all-real.py 174\n",
      "python sart-all-real.py 175\n",
      "python sart-all-real.py 176\n",
      "python sart-all-real.py 177\n",
      "python sart-all-real.py 178\n",
      "python sart-all-real.py 179\n",
      "python sart-all-real.py 180\n",
      "python sart-all-real.py 181\n",
      "python sart-all-real.py 182\n",
      "python sart-all-real.py 183\n",
      "python sart-all-real.py 184\n",
      "python sart-all-real.py 185\n",
      "python sart-all-real.py 186\n",
      "python sart-all-real.py 187\n",
      "python sart-all-real.py 188\n",
      "python sart-all-real.py 189\n",
      "python sart-all-real.py 190\n",
      "python sart-all-real.py 191\n",
      "python sart-all-real.py 192\n",
      "python sart-all-real.py 193\n",
      "python sart-all-real.py 194\n",
      "python sart-all-real.py 195\n",
      "python sart-all-real.py 196\n",
      "python sart-all-real.py 197\n",
      "python sart-all-real.py 198\n",
      "python sart-all-real.py 199\n",
      "python sart-all-real.py 200\n",
      "python sart-all-real.py 201\n",
      "python sart-all-real.py 202\n",
      "python sart-all-real.py 203\n",
      "python sart-all-real.py 204\n",
      "python sart-all-real.py 205\n",
      "python sart-all-real.py 206\n",
      "python sart-all-real.py 207\n",
      "python sart-all-real.py 208\n",
      "python sart-all-real.py 209\n",
      "python sart-all-real.py 210\n",
      "python sart-all-real.py 211\n",
      "python sart-all-real.py 212\n",
      "python sart-all-real.py 213\n",
      "python sart-all-real.py 214\n",
      "python sart-all-real.py 215\n",
      "python sart-all-real.py 216\n",
      "python sart-all-real.py 217\n",
      "python sart-all-real.py 218\n",
      "python sart-all-real.py 219\n",
      "python sart-all-real.py 220\n",
      "python sart-all-real.py 221\n",
      "python sart-all-real.py 222\n"
     ]
    }
   ],
   "source": [
    "for count in range(107, 223):\n",
    "    print('python sart-all-real.py '+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1400, 3600)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For sharpening the reconstructed volume\n",
    "\n",
    "mainfile = '/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/04140608-LE-L-CC_3600x1400x64.4_0.0005_-0.32_216-nosharp.raw'\n",
    "temp = np.fromfile(mainfile, dtype='float32')\n",
    "temp = np.reshape(temp, [64, 1400, 3600])\n",
    "print(temp.shape)\n",
    "    \n",
    "for i in range(64):\n",
    "    a = temp[i]\n",
    "    a = unsharp_mask(a, radius=3, amount=1, preserve_range=True)\n",
    "    temp[i] = a\n",
    "\n",
    "savefile = '/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/04140608-LE-L-CC_3600x1400x64.4_0.0005_-0.32_216-sharp.raw'\n",
    "temp.astype('float32').tofile(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f05b6246390>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(temp[18], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1\n",
      "9.2103405 1.609438\n",
      "(2816, 3584)\n"
     ]
    }
   ],
   "source": [
    "#/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961/LE-L-MLO/04069961.MG.TOMO_BILAT_SEGMENTATION.11.3.2012.11.15.15.54.20.796875.458075.IMA\n",
    "p    = '/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04069961/LE-L-MLO/04069961.MG.TOMO_BILAT_SEGMENTATION.11.3.2012.11.15.15.54.20.796875.458075.IMA'\n",
    "a    = pydicom.dcmread(p)\n",
    "temp = a.pixel_array.T\n",
    "temp[temp == 0] = 1\n",
    "print(np.max(temp.flatten()), np.min(temp.flatten()))\n",
    "temp = np.log(10000)-np.log(temp)\n",
    "#temp[temp>500] = 500\n",
    "print(np.max(temp.flatten()), np.min(temp.flatten()))\n",
    "thresh_min      = threshold_minimum(temp)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f05b799a780>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(temp, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.12.2.1107.5.12.7.1446.30000011080413494334300000023\n"
     ]
    }
   ],
   "source": [
    "# Image Laterality: a.ImageLaterality (left or right)\n",
    "# View Position:    a.ViewPosition (CC or MLO)\n",
    "# SeriesInstanceUID (for determining the Experiment Number)\n",
    "# print(a.ImageLaterality)\n",
    "# print(a.ViewPosition)\n",
    "\n",
    "print(a.StudyInstanceUID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
