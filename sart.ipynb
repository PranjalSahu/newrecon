{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All Imports\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "from numba import cuda\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import glob\n",
    "from skimage import filters\n",
    "from skimage.filters import unsharp_mask, threshold_local, threshold_minimum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "\n",
    "\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import label\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "#from scipy.misc import imresize\n",
    "import pywt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline  \n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "import pywt\n",
    "#import hdf5storage\n",
    "\n",
    "import scipy.io as sio\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "#import pylidc as pl\n",
    "\n",
    "\n",
    "import pywt\n",
    "import numpy as np\n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import skimage.io as io\n",
    "#from sklearn.decomposition import PCA\n",
    "import collections, numpy\n",
    "import warnings\n",
    "from scipy import ndimage, misc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import pymrt as mrt\n",
    "#import pymrt.geometry\n",
    "import ipyvolume as ipv\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import numpy as np\n",
    "from ipdb import set_trace as bp\n",
    "\n",
    "\n",
    "\n",
    "#from image_gen import ImageDataGenerator\n",
    "#from load_data import loadDataMontgomery, loadDataJSRT\n",
    "#from build_model import build_UNet2D_4L\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import numpy\n",
    "import warnings\n",
    "import functools\n",
    "import pickle\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     2,
     32,
     91,
     138,
     143,
     172
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File Reading Code\n",
    "\n",
    "def get_duke_proj(index, lesion):\n",
    "    if lesion:\n",
    "        a       = sio.loadmat(\"/media/dril/My Passport/CHO-DATA/WITH-LESION-MAT/LE/\"+str(index)+\".mat\")[\"head\"]\n",
    "    else:\n",
    "        a       = sio.loadmat(\"/media/dril/My Passport/CHO-DATA/NO-LESION-MAT/LE/\"+str(index)+\".mat\")[\"head\"]\n",
    "    testvol = np.rollaxis(a, 2, 0)\n",
    "    testvol = np.moveaxis(testvol, [0, 1, 2], [0, 2, 1])\n",
    "    \n",
    "    testvol[testvol == 0.80] = 0.81\n",
    "    print(np.unique(testvol.flatten()))\n",
    "    \n",
    "    testvol = testvol/65\n",
    "    proj_arr         = W*testvol\n",
    "    \n",
    "    # All Flags\n",
    "    insert_noise     = 1\n",
    "    if insert_noise:\n",
    "        I0        = 1000\n",
    "        proj      = I0*np.exp(-proj_arr)\n",
    "        proj_noi  = np.random.poisson(proj)\n",
    "        proj_noi[proj_noi == 0] = 1\n",
    "        g_noi                   = np.log(I0) - np.log(proj_noi) # convert back to line integrals \n",
    "        g_noi[g_noi < 0]        = 0\n",
    "        proj_arr = g_noi\n",
    "    \n",
    "    temp_proj = np.reshape(proj_arr, [detCols, num_angles, detRows])\n",
    "    temp_proj = np.rollaxis(temp_proj, 0, 2)\n",
    "    #print(temp_proj.shape)\n",
    "    return temp_proj\n",
    "\n",
    "def load_prj_raw(breast_type):\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    print(BINSx*BINSy, ANGLES, prj_allangle.shape)\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    start_angle = -21.30\n",
    "    \n",
    "    proj_paths = glob.glob(\"/media/dril/Windows/mcgpu1/projections_without_fsm\"+projection_name+\"/*.raw\")\n",
    "    proj_paths.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    \n",
    "    for p in proj_paths:\n",
    "        print(p)\n",
    "        \n",
    "        if '.0000.' in p:\n",
    "            continue\n",
    "        \n",
    "        a    = np.fromfile(p, dtype='float32')#pydicom.dcmread(p)\n",
    "        a    = np.reshape(a, [2, 1504, 3000])\n",
    "        temp = a[0, :, :]#.pixel_array.T\n",
    "        temp = np.log(10000000)-np.log(temp)\n",
    "        \n",
    "        if(0):\n",
    "            # Sharpening filter\n",
    "            temp = unsharp_mask(temp, radius=3, amount=1, preserve_range=True)\n",
    "            thresh_min      = threshold_minimum(temp)\n",
    "            binary_adaptive = temp > thresh_min\n",
    "            temp = np.multiply(temp, binary_adaptive)\n",
    "            \n",
    "        if breast_type == \"right\":\n",
    "            temp = np.fliplr(temp)\n",
    "            temp = temp[-BINSy:]\n",
    "        else:\n",
    "            temp = temp[:BINSy]\n",
    "            temp = np.flipud(temp)\n",
    "            print(temp.shape)\n",
    "        \n",
    "        temp = temp.flatten()\n",
    "        temp = x_y_flip(temp)\n",
    "        \n",
    "        x.append(temp)\n",
    "        y.append(start_angle)\n",
    "        start_angle = start_angle + 1.92\n",
    "    \n",
    "    y = np.array(y)*np.pi/180\n",
    "    \n",
    "    print(\"length x \", len(x), \" \", len(y))\n",
    "    y, x = zip(*sorted(zip(y, x)))\n",
    "    for j in range(len(x)):\n",
    "        print(\"Proj \", j)\n",
    "        flag2 = j\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = x[j][i]\n",
    "    \n",
    "    return prj_allangle, y\n",
    "\n",
    "def load_prj_ima(breast_type):\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    print(BINSx*BINSy, ANGLES, prj_allangle.shape)\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    proj_paths = glob.glob(\"/media/dril/ubuntudata/DBT_recon_data/\"+projection_name+\"/CE*.IMA\")\n",
    "    for p in proj_paths:\n",
    "        if '.0000.' in p:\n",
    "            continue\n",
    "        \n",
    "        a    = pydicom.dcmread(p)\n",
    "        temp = a.pixel_array.T\n",
    "        temp = np.log(10000)-np.log(temp)\n",
    "        # Sharpening filter\n",
    "        temp = unsharp_mask(temp, radius=3, amount=1, preserve_range=True)\n",
    "        thresh_min      = threshold_minimum(temp)\n",
    "        binary_adaptive = temp > thresh_min\n",
    "        temp = np.multiply(temp, binary_adaptive)\n",
    "            \n",
    "        if breast_type == \"right\":\n",
    "            temp = np.fliplr(temp)\n",
    "            temp = temp[-BINSy:]\n",
    "        else:\n",
    "            temp = temp[:BINSy]\n",
    "            temp = np.flipud(temp)\n",
    "        \n",
    "        temp = temp.flatten()\n",
    "        temp = x_y_flip(temp)\n",
    "        \n",
    "        x.append(temp)\n",
    "        y.append(float(a[0x00181530].value))\n",
    "    y = np.array(y)*np.pi/180\n",
    "    \n",
    "    print(\"length x \", len(x), \" \", len(y))\n",
    "    y, x = zip(*sorted(zip(y, x)))\n",
    "    for j in range(len(x)):\n",
    "        print(\"Proj \", j)\n",
    "        flag2 = j\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = x[j][i]\n",
    "    \n",
    "    return prj_allangle, y\n",
    "\n",
    "def import_param():\n",
    "    for i in range(ANGLES):\n",
    "        index[i] = i\n",
    "    return index\n",
    "\n",
    "def load_prj():\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    \n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    scat_allangle = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    for viewangle in range(ANGLES):\n",
    "        s        = viewangle + 1\n",
    "        filename = basepath + filepath+str(s).zfill(4)#+'.raw'\n",
    "        \n",
    "        with open(filename, 'rb') as f:\n",
    "            primary_plus_scatter  = np.fromfile(f, dtype=np.float32)\n",
    "            host_prj_temp1        = primary_plus_scatter[:b_size]\n",
    "            host_prj_temp2        = primary_plus_scatter[b_size:]\n",
    "        \n",
    "        host_prj_1view_temp = x_y_flip(host_prj_temp1)\n",
    "        host_sct_1view_temp = x_y_flip(host_prj_temp2)\n",
    "        \n",
    "        print(host_prj_1view_temp.shape)\n",
    "        \n",
    "        # all angle together\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = host_prj_1view_temp[i]\n",
    "            scat_allangle[flag2*BINSx*BINSy + i] = host_sct_1view_temp[i]\n",
    "        \n",
    "        flag2 = flag2+1\n",
    "    return prj_allangle, scat_allangle\n",
    "    \n",
    "def load_prj_std(data_type):\n",
    "    b_size = BINSx*BINSy\n",
    "    flag2  = 0\n",
    "    \n",
    "    prj_allangle  = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    #scat_allangle = np.zeros(BINSx*BINSy*ANGLES)\n",
    "    \n",
    "    for viewangle in range(ANGLES):\n",
    "        s        = viewangle + 1\n",
    "        \n",
    "        if data_type   == 0:\n",
    "            #filename = basepath + 'OSTR_prelog/'+projection_name+str(s).zfill(4)#+\".raw\"\n",
    "            filename = basepath + 'Projections_Renamed_Seg/'+projection_name+str(s).zfill(4)#+\".raw\"\n",
    "        elif data_type == 1:\n",
    "            filename = basepath + 'OSTR_scatter/'+scatter_name+str(s).zfill(4)#+\".raw\"\n",
    "        else:\n",
    "            filename = basepath + 'OSTR_blank/'+blank_name+str(s).zfill(4)#+\".raw\"\n",
    "        \n",
    "        #print(filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            #data  = np.load(f)\n",
    "            data  = np.fromfile(f, dtype=np.float32)\n",
    "            # If doign SART\n",
    "            #data  = np.log(10000) - np.log(data)\n",
    "            \n",
    "            #print(data.shape)\n",
    "            #data  = np.reshape(data, (1400, 3584))\n",
    "            #data  = data[:1400, :]\n",
    "            #data  = np.flip(data, 0)\n",
    "            #data  = data.flatten()\n",
    "            #print(data.shape)\n",
    "                #data  = np.fromfile(f, dtype=np.float32)\n",
    "        #np.save(filename+'.npy', data)\n",
    "        proj = x_y_flip(data)\n",
    "        \n",
    "        # all angle together\n",
    "        for i in range(0, BINSx*BINSy):\n",
    "            prj_allangle[flag2*BINSx*BINSy + i]  = proj[i]\n",
    "        \n",
    "        flag2 = flag2+1\n",
    "    \n",
    "    return prj_allangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0,
     2,
     55,
     161,
     261,
     371,
     479,
     584,
     686,
     727,
     754,
     856,
     1017,
     1178,
     1341,
     1504,
     1667,
     1830,
     1993,
     2189,
     2339,
     2489,
     2639
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] CNN Models\n",
    "\n",
    "def rating_cnn(input_size = (256, 256, 1)):\n",
    "    filter1 = 4\n",
    "    filter2 = 4\n",
    "    filter3 = 4\n",
    "    filter4 = 4\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same',kernel_initializer = 'glorot_normal')(input1)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #conv1 = Dropout(0.2)(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #conv2 = Dropout(0.2)(conv2)\n",
    "    \n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #conv2 = Dropout(0.2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filter3, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #conv3 = Dropout(0.2)(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(filter4, 3, padding = 'same', kernel_initializer = 'glorot_normal')(conv3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    out1 = Flatten()(conv4)\n",
    "    #out1 = Dropout(0.3)(out1)\n",
    "    \n",
    "    #out2 = Dense(64, kernel_initializer = 'glorot_normal')(out1)\n",
    "    #out2 = LeakyReLU()(out2)\n",
    "    #out2 = Dropout(0.3)(out2)\n",
    "    \n",
    "    #out3 = Dense(32,  kernel_initializer = 'glorot_normal')(out2)\n",
    "    #out3 = LeakyReLU()(out3)\n",
    "    \n",
    "    out3 = Dense(1, activation=\"sigmoid\")(out1)\n",
    "    \n",
    "    model  = Model(input = input1, output = out3)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def unet_double(pretrained_weights = None, input_size = (128, 128, 5), ):\n",
    "    filter1 = 64\n",
    "    filter2 = 128\n",
    "    filter3 = 256\n",
    "    filter4 = 512\n",
    "    filter5 = 1024\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    input3 = Input((1, ))\n",
    "    \n",
    "    hash_val1 = Dense(128, activation='relu')(input2)\n",
    "    hash_val1 = Dense(1, activation='relu')(hash_val1)\n",
    "    \n",
    "    hash_val2 = Dense(128, activation='relu')(input3)\n",
    "    hash_val2 = Dense(1, activation='relu')(hash_val2)\n",
    "    \n",
    "    hash_val = Multiply()([hash_val1, hash_val2])\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(hash_val)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2, input3], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_vanilla(pretrained_weights = None, input_size = (128, 128, 1), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    #input2 = Input((1, ))\n",
    "    \n",
    "    #hash_val = Dense(128, activation='relu')(input2)\n",
    "    #hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    #hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    #input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    #conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    #conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    #conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.1)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    #pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.1)(conv5)\n",
    "    #drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    #conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    #conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    #conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = input1, output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_two(pretrained_weights = None, input_size = (128, 128, 1), ):\n",
    "    filter1 = 64\n",
    "    filter2 = 128\n",
    "    filter3 = 256\n",
    "    filter4 = 512\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(input2)\n",
    "    hash_val = Dropout(0.3)(hash_val)\n",
    "    hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    hash_val1 = Dense(128, activation='relu')(input2)\n",
    "    hash_val1 = Dropout(0.3)(hash_val1)\n",
    "    hash_val1 = Dense(32, activation='relu')(hash_val1)\n",
    "    hash_val1 = Dense(1, activation='relu')(hash_val1)\n",
    "    \n",
    "    input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Dropout(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val1, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Dropout(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val1, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Dropout(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val1, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet(pretrained_weights = None, input_size = (256, 256, 1), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(input2)\n",
    "    hash_val = Dropout(0.3)(hash_val)\n",
    "    hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4, up6], axis = 3)\n",
    "    \n",
    "    \n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Dropout(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Dropout(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Dropout(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_no_dense(pretrained_weights = None, input_size = (256, 256, 1), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input((1, ))\n",
    "    \n",
    "    #hash_val = Dense(128, activation='relu')(input2)\n",
    "    #hash_val = Dropout(0.3)(hash_val)\n",
    "    #hash_val = Dense(32, activation='relu')(hash_val)\n",
    "    hash_val = input2#Dense(1, activation='relu')(hash_val)\n",
    "    \n",
    "    input_mul = Multiply()([hash_val, input1])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.2)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.2)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Dropout(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Dropout(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Dropout(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, :, :, 4])(input1)\n",
    "    input_last  = input1#Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    conv10 = Subtract()([input_last, conv9])\n",
    "    model  = Model(input = [input1, input2], output = conv10)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def unet_lstm(pretrained_weights = None, input_size = (128, 128, 2), ):\n",
    "    filter1 = 32\n",
    "    filter2 = 64\n",
    "    filter3 = 128\n",
    "    filter4 = 256\n",
    "    filter5 = 512\n",
    "    \n",
    "    single_input = Input(input_size)\n",
    "    \n",
    "    input1 = Lambda(lambda x: x[:, :, :, 0])(single_input)\n",
    "    input1 = Reshape([128, 128, 1])(input1)\n",
    "    \n",
    "    input2 = Lambda(lambda x: x[:, :, :, 1])(single_input)\n",
    "    input2 = Flatten()(input2)\n",
    "    input2 = Reshape([1, 128*128])(input2) \n",
    "    input2 = Lambda(lambda x: x[:, :, 0])(input2)\n",
    "    \n",
    "    hash_val = Dense(128, activation='relu')(input2)\n",
    "    hash_val = Dense(1, activation='relu')(hash_val)\n",
    "        \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    conv2 = Multiply()([hash_val, conv2])\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = LeakyReLU(0.2)(conv3)\n",
    "    conv3 = Multiply()([hash_val, conv3])\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    conv4 = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = LeakyReLU(0.2)(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    pool4 = Multiply()([hash_val, pool4])\n",
    "    \n",
    "    conv5 = Conv2D(filter5, 3,  padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    conv5 = Conv2D(filter5, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = LeakyReLU(0.2)(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    drop5 = Multiply()([hash_val, drop5])\n",
    "    \n",
    "    up6    = Conv2D(filter4, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    up6    = LeakyReLU(0.2)(up6)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Conv2D(filter4, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    conv6  = LeakyReLU(0.2)(conv6)\n",
    "    conv6  = Multiply()([hash_val, conv6])\n",
    "        \n",
    "    up7    = Conv2D(filter3, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    up7    = LeakyReLU(0.2)(up7)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Conv2D(filter3, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    conv7  = LeakyReLU(0.2)(conv7)\n",
    "    conv7  = Multiply()([hash_val, conv7])\n",
    "    \n",
    "    up8    = Conv2D(filter2, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    up8    = LeakyReLU(0.2)(up8)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Conv2D(filter2, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    conv8  = LeakyReLU(0.2)(conv8)\n",
    "    conv8  = Multiply()([hash_val, conv8])\n",
    "    \n",
    "    up9    = Conv2D(filter1, 2,  padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    up9    = LeakyReLU(0.2)(up9)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(filter1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    conv9  = Conv2D(1, 3,  padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9  = LeakyReLU(0.2)(conv9)\n",
    "    \n",
    "    #conv10 = Subtract()([input1, conv9])\n",
    "    \n",
    "    model  = Model(input = single_input, output = conv9)\n",
    "    #model.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def lstm_model():\n",
    "    input_size = (5, 128, 128, 1)\n",
    "    \n",
    "    input1     = Input(input_size)\n",
    "    input2     = Input((1, ))\n",
    "    \n",
    "    cnn_model  = unet()\n",
    "    \n",
    "    #time2       = TimeDistributed(cnn_model)(input1)\n",
    "    \n",
    "    lstm_out1   = ConvLSTM2D(filters=32, kernel_size=(3, 3), input_shape=(5, 128, 128, 1),\n",
    "                             padding='same', activation=LeakyReLU(alpha=0.2), return_sequences=True)(input1)\n",
    "    \n",
    "    lstm_out2   = ConvLSTM2D(filters=1, kernel_size=(3, 3), padding='same', activation=LeakyReLU(alpha=0.2),\n",
    "                            return_sequences=False)(lstm_out1)\n",
    "    \n",
    "    input_one = Lambda(lambda x: x[:, 4, :, :, :])(input1)\n",
    "    input_one = Reshape([128, 128, 1])(input_one)\n",
    "    \n",
    "    cnn_out1   = cnn_model([lstm_out2, input2])\n",
    "    \n",
    "    final_out = Subtract()([input_one, cnn_out1])\n",
    "    #lstm_out1   = ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(5, 128, 128, 1),\n",
    "    #                         padding='same', activation=LeakyReLU(alpha=0.2), return_sequences=True)(lstm_out1)\n",
    "    #lstm_out1   = ConvLSTM2D(filters=40, kernel_size=(3, 3), input_shape=(5, 128, 128, 1),\n",
    "    #                         padding='same', activation=LeakyReLU(alpha=0.2), return_sequences=True)(lstm_out1)\n",
    "    #lstm_out2   = ConvLSTM2D(filters=1, kernel_size=(3, 3), padding='same', activation=LeakyReLU(alpha=0.2),\n",
    "    #                         return_sequences=False)(lstm_out1)\n",
    "    \n",
    "    #input_last  = Lambda(lambda x: x[:, 4, :, :, 0])(input1)\n",
    "    #input_last  = Reshape([128, 128, 1])(input_last)\n",
    "    \n",
    "    #conv10      = Subtract()([input1, lstm_out2])\n",
    "    \n",
    "    time_model  = Model(input = [input1, input2], output = final_out)\n",
    "    time_model.compile(optimizer = Adam(lr = 1e-4), \n",
    "                  loss = 'mean_absolute_error', \n",
    "                  metrics = ['mse'])\n",
    "    \n",
    "    return time_model\n",
    "\n",
    "def conv_lstm_model():\n",
    "    seq = Sequential()\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       input_shape=(5, 128, 128, 1),\n",
    "                       padding='same', return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=1, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=False))\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "    #               activation='sigmoid',\n",
    "    #               padding='same', data_format='channels_last'))\n",
    "    \n",
    "    seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def unet_combined(pretrained_weights = None, input_size = (128, 128, 1)):\n",
    "    base_model  = unet()\n",
    "    base_model.load_weights(\"all-data-0-to-7-0.0005-5.h5\")\n",
    "\n",
    "    filter1 = 32\n",
    "    filter2 = 32\n",
    "    filter3 = 32\n",
    "    \n",
    "    input1 = Input(input_size)\n",
    "    input2 = Input(input_size)\n",
    "    \n",
    "    input1_1 = Input((1, ))\n",
    "    input1_2 = Input((1, ))\n",
    "    input1_3 = Input((1, ))\n",
    "    input1_4 = Input((1, ))\n",
    "    input1_5 = Input((1, ))\n",
    "    input1_6 = Input((1, ))\n",
    "    input1_7 = Input((1, ))\n",
    "    \n",
    "    for t in base_model.layers:\n",
    "        t.trainable = False\n",
    "    \n",
    "    w1    = Concatenate()([input1, input2])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(w1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter3, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    \n",
    "    layer_output = base_model.layers[-4].output\n",
    "    \n",
    "    new_model    = Model(base_model.input, layer_output)\n",
    "    \n",
    "    r1 = new_model([input1, input1_1])\n",
    "    r2 = new_model([input1, input1_2])\n",
    "    r3 = new_model([input1, input1_3])\n",
    "    r4 = new_model([input1, input1_4])\n",
    "    r5 = new_model([input1, input1_5])\n",
    "    r6 = new_model([input1, input1_6])\n",
    "    r7 = new_model([input1, input1_7])\n",
    "    \n",
    "#     c1 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 0])(conv1))\n",
    "#     c2 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 1])(conv1))\n",
    "#     c3 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 2])(conv1))\n",
    "#     c4 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 3])(conv1))\n",
    "#     c5 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 4])(conv1))\n",
    "#     c6 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 5])(conv1))\n",
    "#     c7 = Reshape([128, 128, 1])(Lambda(lambda x: x[:, :, :, 6])(conv1))\n",
    "    \n",
    "    \n",
    "#     q1 = Multiply()([conv1, r1])\n",
    "#     q2 = Multiply()([conv1, r2])\n",
    "#     q3 = Multiply()([conv1, r3])\n",
    "#     q4 = Multiply()([conv1, r4])\n",
    "#     q5 = Multiply()([conv1, r5])\n",
    "#     q6 = Multiply()([conv1, r6])\n",
    "#     q7 = Multiply()([conv1, r7])\n",
    "    \n",
    "    #ut   = Add()([q1, q2, q3, q4, q5, q6, q7])\n",
    "    #ut   = Add()([c1, c2, c3, c4, c5, c6, c7])\n",
    "    out   = Add()([r1, r2, r3, r4, r5, r6, r7])\n",
    "    out   = Multiply()([conv1, out])\n",
    "    \n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(filter1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    conv1 = Conv2D(1, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    out   = LeakyReLU(0.2)(conv1)\n",
    "    \n",
    "    #conv1 = Multiply()([hash_val, conv1])\n",
    "    \n",
    "    model1  = Model(input = [input1, input2, input1_1, input1_2, input1_3, input1_4, input1_5, input1_6, input1_7], output = out)\n",
    "    \n",
    "    model1.compile(optimizer = Adam(lr = 1e-4), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "    return model1\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# in1 = np.ones([10, 1, 256, 256])\n",
    "# in2 = np.ones([10, 1])\n",
    "\n",
    "# in1 = torch.tensor(in1, device=device).float()\n",
    "# in2 = torch.tensor(in2, device=device).float()\n",
    "\n",
    "\n",
    "# Define model\n",
    "class MyUnetGN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "        \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU())\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter1//2, filter1))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter2//2, filter2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter3//2, filter3))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter4//2, filter4))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter5//2, filter5))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter4//2, filter4))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter3//2, filter3))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter2//2, filter2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter1//2, filter1))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter4//2, filter4))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter3//2, filter3))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter2//2, filter2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.GroupNorm(filter1//2, filter1))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.1)\n",
    "        self.d2 = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "        \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU())\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter1))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter3))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter4))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter5))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter4))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter3))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter1))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter4))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter3))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(filter2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2filter1\n",
    "            nn.BatchNorm2d(filter1))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.1)\n",
    "        self.d2 = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice1_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val1)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val1)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val1)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class MyUnetSlice2_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val1 = self.dense_block(v)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val1)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val1)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val1)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val1 = self.dense_block(y)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val = self.dense_block(v)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val1)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val1)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val1)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnetSlice4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block1 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block2 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block3 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block4 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.dense_block5 = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        #self.d1 = nn.Dropout(0.5)\n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y, v):\n",
    "        hash_val1 = self.dense_block1(y)\n",
    "        hash_val1 = hash_val1.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val2 = self.dense_block2(y)\n",
    "        hash_val2 = hash_val2.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val3 = self.dense_block3(y)\n",
    "        hash_val3 = hash_val3.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val4 = self.dense_block4(y)\n",
    "        hash_val4 = hash_val4.view(-1, 1, 1, 1)\n",
    "        \n",
    "        hash_val5 = self.dense_block5(y)\n",
    "        hash_val5 = hash_val5.view(-1, 1, 1, 1)\n",
    "        \n",
    "        #hash_val = torch.mul(hash_val, hash_val1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val1)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val2)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val3)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val4)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val4)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val3)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val2)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet_double(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 64\n",
    "        filter2 = 128\n",
    "        filter3 = 256\n",
    "        filter4 = 512\n",
    "        filter5 = 1024\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(0.2),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU(0.2)\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class RatingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            #nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            #nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.BatchNorm2d(4),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        \n",
    "        self.out = nn.Sequential(nn.Linear(256, 1),\n",
    "                                 nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        \n",
    "        x = x.view(-1, 256)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "#model = MyUnet()\n",
    "#model.cuda()\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "#model.apply(init_normal)\n",
    "\n",
    "#unet_model = MyUnetGN()\n",
    "#unet_model.cuda()\n",
    "\n",
    "# rating_cnn = RatingModel()\n",
    "# rating_cnn.cuda()\n",
    "\n",
    "#rating_cnn.apply(init_normal)\n",
    "#summary(unet_model, [(1, 256, 256), (1, 1)])\n",
    "\n",
    "# m = rating_cnn()\n",
    "# print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     175
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Getting patches for each lesion for training the calibaration CNN with lesions\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "allfiles = glob.glob(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_annotation/index-b-*.npy\")\n",
    "allindex = []\n",
    "for f in allfiles:\n",
    "    index = int(f.split(\"/\")[-1].split(\"-\")[-1][:-4])\n",
    "    allindex.append(index)\n",
    "\n",
    "patches = {}\n",
    "values  = {}\n",
    "\n",
    "#print(val_list)\n",
    "\n",
    "\n",
    "for img_counter in range(1, 20):\n",
    "    counter = 0\n",
    "    for k in allindex:\n",
    "        #print(h[k][0], h[k], test_list)\n",
    "\n",
    "        if h[k][0] not in test_list:\n",
    "            continue\n",
    "\n",
    "        counter = counter+1\n",
    "        if counter < img_counter:\n",
    "            continue\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        all_outputs = []\n",
    "        all_ssim = []\n",
    "        all_haar = []\n",
    "\n",
    "        patches = []\n",
    "        values  = []\n",
    "\n",
    "        path = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(h[k][0])+\".raw\")[0]\n",
    "        vol  = np.fromfile(path, dtype=\"float32\")\n",
    "        vol  = np.reshape(vol, [64, 1200, 3000])\n",
    "        \n",
    "        px = random.randint(500, 800)\n",
    "        rx = random.randint(1500, 2000)\n",
    "        tx = random.randint(25, 35)\n",
    "        \n",
    "        for tp in range(1, 2):\n",
    "            #tx = 0\n",
    "            #rx = 0\n",
    "            temp = vol[tx, -128+px:128+px, -128+rx:128+rx]\n",
    "            #temp = vol[h[k][3]+tp+tx, h[k][2]-128+rx:h[k][2]+128+rx, h[k][1]-128+rx:h[k][1]+128+rx]\n",
    "            #print(temp.shape)\n",
    "            if temp.shape[0] == 256 and temp.shape[1] == 256:\n",
    "                x.append(temp)\n",
    "                values.append(0)\n",
    "\n",
    "        locations_array = []\n",
    "        locations_flag  = 0\n",
    "\n",
    "        allpaths  = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(h[k][0])+\".raw.npy\")\n",
    "        allpaths.sort()\n",
    "\n",
    "        print(allpaths[0])\n",
    "\n",
    "        for t in allpaths:\n",
    "            #print(t)\n",
    "            s = float(t.split(\"/\")[-1].split(\"-\")[-1].split(\"_\")[0])\n",
    "\n",
    "            vol  = np.load(t)\n",
    "            vol  = np.reshape(vol, [64, 1200, 3000])\n",
    "\n",
    "            temp_count      = 0\n",
    "\n",
    "            for tp in range(1, 2):\n",
    "                #for dx in dx_array:\n",
    "                    #for dy in dy_array:\n",
    "                #rx = 0\n",
    "                #tx = 0\n",
    "                temp = vol[tx, -128+px:128+px, -128+rx:128+rx]\n",
    "                #temp = vol[h[k][3]+tp+tx, h[k][2]-128+rx:h[k][2]+128+rx, h[k][1]-128+rx:h[k][1]+128+rx]\n",
    "                #print(temp.shape)\n",
    "                #temp = vol[h[k][3]+tp, h[k][2]-128+dx:h[k][2]+128+dx, h[k][1]-128+dy:h[k][1]+128+dy]\n",
    "                #print(temp.shape)\n",
    "                if temp.shape[0] == 256 and temp.shape[1] == 256:\n",
    "                    x.append(temp)\n",
    "                    values.append(s)\n",
    "\n",
    "                    #print(s)\n",
    "                    pp1 = np.expand_dims(x[0], 0)\n",
    "                    pp1 = np.expand_dims(pp1,  0)\n",
    "                    pv1 = np.expand_dims(-1*s, 0)\n",
    "                    #print(pp1.shape, pv1.shape)\n",
    "\n",
    "                    pp1 = torch.tensor(pp1, device=device).float()\n",
    "                    pv1 = torch.tensor(pv1, device=device).float()\n",
    "\n",
    "                    result = unet_model.forward(pp1, pv1).data.cpu().numpy()\n",
    "                    all_outputs.append(result)\n",
    "                    #ssim_temp = measure.compare_ssim(result[0, 0, :, :].astype('float32'), temp.astype('float32'), data_range=temp.max() - temp.min())\n",
    "                    t1 = np.min(result[0, 0, :, :].flatten())\n",
    "                    t2 = np.max(result[0, 0, :, :].flatten())\n",
    "                    distorted_image  = (result[0, 0, :, :]-t1)*255/(t2-t1)\n",
    "\n",
    "                    t1 = np.min(temp.flatten())\n",
    "                    t2 = np.max(temp.flatten())\n",
    "                    reference_image = (temp-t1)*255/(t2-t1)\n",
    "\n",
    "                    ssim_temp = measure.compare_ssim(reference_image.astype('float32'), distorted_image.astype('float32'), 255)\n",
    "                    #print(temp.max(), temp.min(), temp.max() - temp.min())\n",
    "\n",
    "                    ssim_temp1 = haar_psi_numpy(reference_image, distorted_image, preprocess_with_subsampling = True)[0]\n",
    "                    all_haar.append(ssim_temp1)\n",
    "                    all_ssim.append(ssim_temp)\n",
    "                    #print(temp.shape, result.shape, ssim_temp, ssim_temp1)\n",
    "\n",
    "        x      = np.array(x)\n",
    "        values = np.array(values)\n",
    "        all_outputs = np.array(all_outputs)\n",
    "        all_outputs = all_outputs[:, 0, 0, :, :]\n",
    "\n",
    "        #print(x.shape, values.shape, all_outputs.shape)\n",
    "        break\n",
    "\n",
    "    #plt.figure(figsize=(40,20))\n",
    "    #plt.axis('off')\n",
    "    tp1 = [x[1], x[3], x[4], x[5], x[6], x[7]]\n",
    "    tv1 = np.array([values[1], values[3], values[4], values[5], values[6], values[7]])\n",
    "    tv1[tv1 > 0.6] = 0.6\n",
    "\n",
    "    ssim_arr = [all_ssim[1-1], all_ssim[3-1], all_ssim[4-1], all_ssim[5-1], all_ssim[6-1], all_ssim[7-1]]\n",
    "    haar_arr = [all_haar[1-1], all_haar[3-1], all_haar[4-1], all_haar[5-1], all_haar[6-1], all_haar[7-1]]\n",
    "\n",
    "    #plt.imshow(np.concatenate(tp1, axis=0).T, cmap='gray')\n",
    "\n",
    "    print(values)\n",
    "    print(tv1)\n",
    "    print(ssim_arr)\n",
    "    print(haar_arr)\n",
    "\n",
    "    tp2 = [all_outputs[1-1], all_outputs[3-1], all_outputs[4-1], all_outputs[5-1], all_outputs[6-1], all_outputs[7-1]]\n",
    "\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = [9.6, 3.4]\n",
    "\n",
    "    #gs1 = gridspec.GridSpec(1, 8)\n",
    "    #gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "    top_title = \"\"\n",
    "    for i in range(1, len(tp2)+1):\n",
    "        if i == 1:\n",
    "            top_title = top_title+r'$\\beta$='+str(np.round(tv1[i-1], 3))\n",
    "        else:\n",
    "            top_title = top_title+r'               $\\beta$='+str(np.round(tv1[i-1], 3))\n",
    "\n",
    "\n",
    "    bottom_title = \"\"\n",
    "    for i in range(1, len(tp2)+1):\n",
    "        if i == 1:\n",
    "            bottom_title = bottom_title+\"SSIM=\"+str(np.round(ssim_arr[i-1], 3))+\",HaarPSI=\"+str(np.round(haar_arr[i-1], 3))\n",
    "        else:\n",
    "            bottom_title = bottom_title+\"   SSIM=\"+str(np.round(ssim_arr[i-1], 3))+\",HaarPSI=\"+str(np.round(haar_arr[i-1], 3))\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        #if i == 1:\n",
    "        axs[0].set_ylabel(\"Ground Truth\")\n",
    "        axs[0].imshow(np.concatenate(tp1).T, cmap='gray')\n",
    "        axs[0].set_title(top_title, y=0.95, fontsize=10)\n",
    "        axs[0].set_xticks([], [])\n",
    "        axs[0].set_yticks([], [])\n",
    "        axs[0].spines['top'].set_visible(False)\n",
    "        axs[0].spines['bottom'].set_visible(False)\n",
    "        axs[0].spines['left'].set_visible(False)\n",
    "        axs[0].spines['right'].set_visible(False)\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        #if i == 1:\n",
    "        axs[1].set_ylabel(\"U-Net Output\")\n",
    "        axs[1].imshow(np.concatenate(tp2).T, cmap='gray')\n",
    "        axs[1].set_title(bottom_title, y=-0.15, fontsize=6)\n",
    "        axs[1].set_xticks([], [])\n",
    "        axs[1].set_yticks([], [])\n",
    "        axs[1].spines['top'].set_visible(False)\n",
    "        axs[1].spines['bottom'].set_visible(False)\n",
    "        axs[1].spines['left'].set_visible(False)\n",
    "        axs[1].spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False)\n",
    "    plt.tick_params(\n",
    "        axis='y',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        right=False,\n",
    "        labelbottom=False)\n",
    "    plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')\n",
    "\n",
    "    f.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.savefig('compare_supp_random4_'+str(img_counter)+'.png', dpi=300, bbox_inches = 'tight',\n",
    "        pad_inches = 0.1)\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plt.show(block=True)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(40,20))\n",
    "# print(values)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(np.concatenate(x[:-2], axis=0).T, cmap='gray')\n",
    "# print(x.shape, values.shape)\n",
    "#np.save(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_cho_data/x_\"+str(k)+\".npy\", x)\n",
    "#np.save(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_cho_data/y_\"+str(k)+\".npy\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] For optimizing the Tuning parameter using the PyTorch Model\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "rating_cnn = RatingModel()\n",
    "rating_cnn.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "rating_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/rating_pytorch.pt')\n",
    "rating_cnn.load_state_dict(rating_weights)\n",
    "\n",
    "\n",
    "#unet_model.eval()\n",
    "#rating_cnn.eval()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device  = torch.device(\"cuda:0\")\n",
    "\n",
    "targets = torch.tensor(np.ones([1, 1]), device=device).float()\n",
    "\n",
    "in1 = x[0, :, :]#np.ones([1, 1, 256, 256]) #np.ones([256, 256])\n",
    "in1 = np.expand_dims(in1, axis=0)\n",
    "in1 = np.expand_dims(in1, axis=1)\n",
    "in1 = torch.tensor(in1, device=device).float()\n",
    "\n",
    "in2 = Variable(torch.tensor(-0.2*np.ones([1, 1], dtype='float32')).cuda(), requires_grad=True)\n",
    "\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.Adam([input_img.requires_grad_()], lr=0.001)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = get_input_optimizer(in2)\n",
    "\n",
    "first_image = 0\n",
    "best_image  = 0\n",
    "best = 1000\n",
    "print('Optimizing..')\n",
    "run = [0]\n",
    "\n",
    "s1 = time.time()\n",
    "while run[0] <= 100:\n",
    "    optimizer.zero_grad()\n",
    "    out1 = unet_model(in1, in2)\n",
    "    out2 = rating_cnn(out1)\n",
    "    \n",
    "    if run[0] == 0:\n",
    "        first_image = out1.data.cpu().numpy()\n",
    "        \n",
    "    loss = criterion(out2, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    run[0] += 1\n",
    "    #if run[0] % 10 == 0:\n",
    "        #print(np.mean(out1.data.cpu().numpy().flatten()))\n",
    "    if loss.item() < best:\n",
    "        best       = loss.item()\n",
    "        best_image = out1.data.cpu().numpy()\n",
    "        #print(\"Loss is \", loss.item(), \" variable \", in2.item(), \" out2 \", out2.item())#, \"out1 \", out1.item())#np.mean(out1.item().flatten()))\n",
    "    optimizer.step()\n",
    "s2 = time.time()\n",
    "print(s2-s1)\n",
    "\n",
    "# Good counters = 11, 12 (16 is best)\n",
    "plt.axis('off')\n",
    "plt.imshow(np.concatenate([x[0, :, :], first_image[0, 0, :, :], best_image[0, 0, :, :]], axis=-1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For filtering the entire slice using PyTorch Model\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "\n",
    "allfiles = glob.glob(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_annotation/index-b-*.npy\")\n",
    "allindex = []\n",
    "for f in allfiles:\n",
    "    index = int(f.split(\"/\")[-1].split(\"-\")[-1][:-4])\n",
    "    allindex.append(index)\n",
    "\n",
    "patches = {}\n",
    "values  = {}\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.Adam([input_img.requires_grad_()], lr=0.001)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "opt_counter = 0\n",
    "for img_counter in range(1, 20):\n",
    "    counter = 0\n",
    "    for k in allindex:\n",
    "        #print(h[k][0], h[k], test_list)\n",
    "\n",
    "        if h[k][0] not in test_list:\n",
    "            continue\n",
    "\n",
    "        counter = counter+1\n",
    "        if counter < img_counter:\n",
    "            continue\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        all_outputs = []\n",
    "        all_ssim    = []\n",
    "        all_haar    = []\n",
    "\n",
    "        patches = []\n",
    "        values  = []\n",
    "\n",
    "        path = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(h[k][0])+\".raw\")[0]\n",
    "        vol  = np.fromfile(path, dtype=\"float32\")\n",
    "        vol  = np.reshape(vol, [64, 1200, 3000])\n",
    "        print(path)\n",
    "        \n",
    "        for tp in range(1, 2):\n",
    "            temp = vol[h[k][3]+tp, :, :]\n",
    "            #print(temp.shape)\n",
    "        \n",
    "        #1280, 3072\n",
    "        image_slice = np.pad(temp, ((40, 40), (36, 36)), 'constant', constant_values=(0, 0))\n",
    "        #print(image_slice.shape)\n",
    "        \n",
    "        temp_all = []\n",
    "        for tk in range(5):\n",
    "            temp_row = []\n",
    "            for tj in range(12):\n",
    "                img = image_slice[tk*256:(tk+1)*256, tj*256:(tj+1)*256]\n",
    "                \n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                t1  = np.expand_dims(img, axis=1)\n",
    "                \n",
    "                t2 = np.expand_dims(-1*np.ones([1, 1])*0.125, axis=-1)\n",
    "                \n",
    "                x1      = torch.tensor(t1, device=device).float()\n",
    "                values1 = torch.tensor(t2, device=device).float()\n",
    "                \n",
    "                output = unet_model.forward(x1, values1)\n",
    "                output = output.data.cpu().numpy()\n",
    "                output = output[0, 0, :, :]\n",
    "                temp_row.append(output)\n",
    "            temp_row = np.concatenate(temp_row, axis=1)\n",
    "            temp_all.append(temp_row)\n",
    "        temp_all1 = np.concatenate(temp_all, axis=0)\n",
    "        \n",
    "        temp_all = []\n",
    "        for tk in range(5):\n",
    "            temp_row = []\n",
    "            for tj in range(12):\n",
    "                img = image_slice[tk*256:(tk+1)*256, tj*256:(tj+1)*256]\n",
    "                \n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                t1  = np.expand_dims(img, axis=1)\n",
    "                \n",
    "                t2 = np.expand_dims(-1*np.ones([1, 1])*0.2, axis=-1)\n",
    "                \n",
    "                x1      = torch.tensor(t1, device=device).float()\n",
    "                values1 = torch.tensor(t2, device=device).float()\n",
    "                \n",
    "                output = unet_model.forward(x1, values1)\n",
    "                output = output.data.cpu().numpy()\n",
    "                output = output[0, 0, :, :]\n",
    "                temp_row.append(output)\n",
    "            temp_row = np.concatenate(temp_row, axis=1)\n",
    "            temp_all.append(temp_row)\n",
    "        temp_all2 = np.concatenate(temp_all, axis=0)\n",
    "        \n",
    "        temp_all2   = temp_all2[40:-40, 36:-36]\n",
    "        temp_all1   = temp_all1[40:-40, 36:-36]\n",
    "        image_slice = image_slice[40:-40, 36:-36]\n",
    "        \n",
    "        image_slice.astype('float32').tofile('image_slice'+str(img_counter)+'_3000x1200.raw')\n",
    "        temp_all1.astype('float32').tofile('result_slice'+str(img_counter)+'_3000x1200.raw')\n",
    "        temp_all2.astype('float32').tofile('result_2slice'+str(img_counter)+'_3000x1200.raw')\n",
    "        \n",
    "        print(temp_all1.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For predicting the result PyTorch Model for Time calculation\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "unet_model = MyUnet()\n",
    "unet_model.cuda()\n",
    "\n",
    "rating_cnn = RatingModel()\n",
    "rating_cnn.cuda()\n",
    "\n",
    "unet_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/unet_pytorch.pt')\n",
    "unet_model.load_state_dict(unet_weights)\n",
    "\n",
    "rating_weights = torch.load('/media/dril/Windows/newrecon2/newrecon/rating_pytorch.pt')\n",
    "rating_cnn.load_state_dict(rating_weights)\n",
    "\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "#all_outputs.append(x[0, :, :])\n",
    "import time\n",
    "\n",
    "x = np.random.rand(3515, 256, 256)\n",
    "s1 = time.time()\n",
    "for i in range(1, 219):\n",
    "    t1 = np.expand_dims(x[i*16:(i+1)*16, :, :], axis=1)\n",
    "    #1 = [x[i*8:(i+1)*8, :, :]]\n",
    "    t2 = np.expand_dims(-1*np.ones([16, 1])*0.2, axis=-1)\n",
    "    #t2 = np.expand_dims(-1*np.array([values[i]]), axis=-1)\n",
    "    #print(values[i])\n",
    "    \n",
    "    x1      = torch.tensor(t1, device=device).float()\n",
    "    values1 = torch.tensor(t2, device=device).float()\n",
    "    \n",
    "    output = unet_model.forward(x1, values1)\n",
    "    output = output.data.cpu().numpy()\n",
    "    \n",
    "    all_outputs.append(output[0, 0, :, :])\n",
    "all_outputs = np.array(all_outputs)\n",
    "\n",
    "s2 = time.time()\n",
    "\n",
    "print(s2-s1)\n",
    "print(all_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the results\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "o1 = np.concatenate(all_outputs[:7, :, :], axis=0).T\n",
    "print(o1.shape)\n",
    "\n",
    "o2 = np.concatenate(x[1:8, :, :]).T\n",
    "print(o2.shape)\n",
    "\n",
    "#print(o1.shape, o2.shape)\n",
    "\n",
    "together = np.concatenate([o1, o2], axis=0)\n",
    "#print(o1.shape, o2.shape, together.shape)\n",
    "print(values[1:])\n",
    "plt.imshow(together, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     17
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Comparing the Noise Power Spectrum\n",
    "\n",
    "from scipy import fftpack\n",
    "import pyfits\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import radialProfile\n",
    "\n",
    "index  = 2\n",
    "image1 = o1[:, index*256:(index+1)*256]\n",
    "image2 = o2[:, index*256:(index+1)*256]\n",
    "\n",
    "index  = 5\n",
    "image3 = o1[:, index*256:(index+1)*256]\n",
    "image4 = o2[:, index*256:(index+1)*256]\n",
    "\n",
    "\n",
    "def get_power_spectrum(image):    \n",
    "    # Take the fourier transform of the image.\n",
    "    F1 = fftpack.fft2(image)\n",
    "    F2 = fftpack.fftshift(F1)\n",
    "    \n",
    "    # Calculate a 2D power spectrum\n",
    "    psf2D = np.abs( F2 )**2\n",
    "    \n",
    "    # Calculate the azimuthally averaged 1D power spectrum\n",
    "    psf1D = radialProfile.azimuthalAverage(psf2D)\n",
    "    return psf1D\n",
    "\n",
    "t1 = get_power_spectrum(image1)\n",
    "t2 = get_power_spectrum(image2)\n",
    "\n",
    "t3 = get_power_spectrum(image3)\n",
    "t4 = get_power_spectrum(image4)\n",
    "\n",
    "\n",
    "py.figure(3)\n",
    "py.clf()\n",
    "\n",
    "py.semilogy( t1, 'r--', label=r'U-Net             $\\beta = 0.154$')\n",
    "py.semilogy( t3, 'b--', label=r'U-Net             $\\beta = 0.454$')\n",
    "\n",
    "py.semilogy( t2, label=r'Ground Truth $\\beta = 0.154$')\n",
    "py.semilogy( t4, label=r'Ground Truth $\\beta = 0.454$')\n",
    "\n",
    "\n",
    "py.xlabel(\"Spatial Frequency\", fontsize=24)\n",
    "py.ylabel(\"Power Spectrum\",fontsize=24)\n",
    "py.legend(fontsize=20)\n",
    "py.xticks(fontsize=16)\n",
    "py.yticks(fontsize=16)\n",
    "py.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For getting the SSIM metric\n",
    "\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "#from skimage import structural_similarity as ssim\n",
    "from skimage import measure\n",
    "import glob\n",
    "\n",
    "import importlib\n",
    "importlib.reload(haarPsi)\n",
    "\n",
    "#import \n",
    "from haarPsi import haar_psi_numpy\n",
    "\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "ssim_array = {}\n",
    "ssim_array[0.1] = []\n",
    "ssim_array[0.2] = []\n",
    "ssim_array[0.3] = []\n",
    "ssim_array[0.4] = []\n",
    "ssim_array[0.5] = []\n",
    "ssim_array[0.6] = []\n",
    "\n",
    "\n",
    "mae_array = {}\n",
    "mae_array[0.1] = []\n",
    "mae_array[0.2] = []\n",
    "mae_array[0.3] = []\n",
    "mae_array[0.4] = []\n",
    "mae_array[0.5] = []\n",
    "mae_array[0.6] = []\n",
    "\n",
    "def my_mae(x, y):\n",
    "    return np.mean(np.abs(x-y))\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "checkit = []\n",
    "\n",
    "flag = False\n",
    "\n",
    "for t in test_list:\n",
    "    a        = np.load(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/PATCHES/\"+str(t)+\"_0.npy\")\n",
    "    allfiles = glob.glob(\"/media/dril/My Passport/REAL-DBT-PROJECTIONS/PATCHES/\"+str(t)+\"_-*.npy\")\n",
    "    \n",
    "    for f in allfiles:\n",
    "        value = -1*float(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "        b     = np.load(f)\n",
    "        print(f, value)\n",
    "        \n",
    "        for index in range(50):\n",
    "            b1    = b[index, :, :, 0]\n",
    "            \n",
    "            t1    = a[index, :, :, 0]\n",
    "            t1    = np.expand_dims(t1, 0)\n",
    "            t1    = np.expand_dims(t1, 1)\n",
    "            \n",
    "            values  = -1*value*np.ones([1, 1])\n",
    "            \n",
    "            x1      = torch.tensor(t1, device=device).float()\n",
    "            values  = torch.tensor(values, device=device).float()\n",
    "        \n",
    "            output = unet_model.forward(x1, values)\n",
    "            pred   = output.data.cpu().numpy()\n",
    "            \n",
    "            t1 = np.min(b1.flatten())\n",
    "            t2 = np.max(b1.flatten())\n",
    "            reference_image = (b1-t1)*255/(t2-t1)\n",
    "            \n",
    "            t1 = np.min(pred[0, 0, :, :].flatten())\n",
    "            t2 = np.max(pred[0, 0, :, :].flatten())\n",
    "            distorted_image = (pred[0, 0, :, :]-t1)*255/(t2-t1)\n",
    "            #print(np.min(distorted_image), np.max(distorted_image), np.max(reference_image), np.min(reference_image))\n",
    "            \n",
    "            ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "            #ssim_temp = measure.compare_ssim(pred[0, 0, :, :].astype('float16'), b1.astype('float16'))\n",
    "            #ssim_temp = measure.compare_ssim(pred[0, 0, :, :].astype('float32'), b1.astype('float32'), data_range=b1.max() - b1.min())\n",
    "            #ssim_temp = measure.compare_ssim(pred[0, 0, :, :].astype('float32'), b1.astype('float32'), data_range=pred[0, 0, :, :].max() - pred[0, 0, :, :].min())\n",
    "            \n",
    "            #ssim_temp = haar_psi_numpy(reference_image, distorted_image, preprocess_with_subsampling = True)[0]\n",
    "            if ssim_temp < 0.6 and value < 0.6:\n",
    "                import random\n",
    "                lp = random.randint(0, 100)\n",
    "                if lp > 90:\n",
    "                    flag       = True\n",
    "                    ssim_temp1 = haar_psi_numpy(reference_image.astype('float32'), distorted_image.astype('float32'),  preprocess_with_subsampling = True)[0]\n",
    "                    print(ssim_temp, value, ssim_temp1)\n",
    "                    checkit.append([reference_image, distorted_image])\n",
    "                    break\n",
    "            #print(ssim_temp[0], ssim_temp[1].shape)\n",
    "            \n",
    "            mae_temp  = my_mae(pred[0, 0, :, :], b1)\n",
    "                \n",
    "            if value < 0.1:\n",
    "                ssim_array[0.1].append(ssim_temp)\n",
    "                mae_array[0.1].append(mae_temp)\n",
    "            elif value < 0.2:\n",
    "                ssim_array[0.2].append(ssim_temp)\n",
    "                mae_array[0.2].append(mae_temp)    \n",
    "            elif  value < 0.3:\n",
    "                ssim_array[0.3].append(ssim_temp)\n",
    "                mae_array[0.3].append(mae_temp)    \n",
    "            elif  value < 0.4:\n",
    "                ssim_array[0.4].append(ssim_temp)\n",
    "                mae_array[0.4].append(mae_temp)    \n",
    "            elif value < 0.5:\n",
    "                ssim_array[0.5].append(ssim_temp)\n",
    "                mae_array[0.5].append(mae_temp)    \n",
    "            elif value < 0.6:\n",
    "                ssim_array[0.6].append(ssim_temp)\n",
    "                mae_array[0.6].append(mae_temp)\n",
    "        if flag:\n",
    "            break\n",
    "    if flag:\n",
    "        break\n",
    "\n",
    "values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "for t in values:\n",
    "    print(t, np.mean(ssim_array[t]), np.mean(mae_array[t]), np.std(ssim_array[t]), np.std(mae_array[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting Power Spectrum\n",
    "\n",
    "from scipy import fftpack\n",
    "import pyfits\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import radialProfile\n",
    "\n",
    "image = #pyfits.getdata(myimage.fits)\n",
    "\n",
    "# Take the fourier transform of the image.\n",
    "F1 = fftpack.fft2(image)\n",
    "F2 = fftpack.fftshift( F1 )from scipy import fftpack\n",
    "import pyfits\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import radialProfile\n",
    "\n",
    "image = #pyfits.getdata(myimage.fits)\n",
    "\n",
    "# Take the fourier transform of the image.\n",
    "F1 = fftpack.fft2(image)\n",
    "F2 = fftpack.fftshift( F1 )\n",
    "\n",
    "# Calculate a 2D power spectrum\n",
    "psd2D = np.abs( F2 )**2\n",
    "\n",
    "# Calculate the azimuthally averaged 1D power spectrum\n",
    "psd1D = radialProfile.azimuthalAverage(psd2D)\n",
    "\n",
    "# Now plot up both\n",
    "py.figure(1)\n",
    "py.clf()\n",
    "py.imshow( np.log10( image ), cmap=py.cm.Greys)\n",
    "\n",
    "py.figure(2)\n",
    "py.clf()\n",
    "py.imshow( np.log10( psf2D ))\n",
    "\n",
    "py.figure(3)\n",
    "py.clf()\n",
    "py.semilogy( psf1D )\n",
    "py.xlabel(Spatial Frequency)\n",
    "py.ylabel(Power Spectrum)\n",
    "\n",
    "py.show()\n",
    "\n",
    "# Calculate a 2D power spectrum\n",
    "psd2D = np.abs( F2 )**2\n",
    "\n",
    "# Calculate the azimuthally averaged 1D power spectrum\n",
    "psd1D = radialProfile.azimuthalAverage(psd2D)\n",
    "\n",
    "# Now plot up both\n",
    "py.figure(1)\n",
    "py.clf()\n",
    "py.imshow( np.log10( image ), cmap=py.cm.Greys)\n",
    "\n",
    "py.figure(2)\n",
    "py.clf()\n",
    "py.imshow( np.log10( psf2D ))\n",
    "\n",
    "py.figure(3)\n",
    "py.clf()\n",
    "py.semilogy( psf1D )\n",
    "py.xlabel(Spatial Frequency)\n",
    "py.ylabel(Power Spectrum)\n",
    "\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     29
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Rating CNN in Pytorch\n",
    "\n",
    "trainx = np.zeros([160, 256, 256, 1])\n",
    "trainy = np.zeros([160, 1])\n",
    "valx   = np.zeros([230, 256, 256, 1])\n",
    "valy   = np.zeros([230, 1])\n",
    "\n",
    "train_list = [1, 3, 6, 7, 11, 12, 13, 16, 17, 31, 33, 35, 37, 39, 41, 43, 50, 52, 55,  68, 69, 70, 71, 72, 73, 76]\n",
    "val_list   = [19, 21, 23, 25, 27, 29, 61, 62, 64, 65]\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "traincount = 0\n",
    "valcount   = 0\n",
    "\n",
    "allfiles = glob.glob(\"/media/dril/Windows/newrecon2/newrecon/dbt_real_ratings/x_*.npy\")\n",
    "for f in allfiles:\n",
    "    #print(f)\n",
    "    \n",
    "    x1 = np.load(f)\n",
    "    y1 = -1*np.load(f.replace(\"x_\", \"z_\"))\n",
    "    \n",
    "    k = int(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "    #print(k, h[k])\n",
    "    \n",
    "    x1 = np.expand_dims(x1, axis=-1)\n",
    "    y1 = np.expand_dims(y1, axis=-1)\n",
    "    \n",
    "    #print(x1.shape, y1.shape)\n",
    "    \n",
    "    if h[k][0] in train_list:\n",
    "        #print(x1.shape, y1.shape, \"Train\")\n",
    "        if x1.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        trainx[traincount:traincount+x1.shape[0], :, :, :] = x1\n",
    "        trainy[traincount:traincount+x1.shape[0], :] = y1\n",
    "        #trainx.append(x1)\n",
    "        #trainy.append(y1)\n",
    "        traincount = traincount+x1.shape[0]\n",
    "    elif h[k][0] in test_list:\n",
    "        print(f)\n",
    "        #print(x1.shape, y1.shape, \"Val\")\n",
    "        if x1.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        #valx[valcount:valcount+x1.shape[0], :, :, :] = x1\n",
    "        #valy[valcount:valcount+x1.shape[0], :] = y1\n",
    "        #valx.append(x1)\n",
    "        #valy.append(y1)\n",
    "        valcount = valcount+x1.shape[0]\n",
    "\n",
    "print(traincount, valcount)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Code to get the patches\n",
    "import numpy as np\n",
    "\n",
    "#todo = [15, 16, 19, 20, 27, 29, 31, 35]\n",
    "\n",
    "train_list = [1, 3, 6, 7, 11, 12, 13, 16, 17, 31, 33, 35, 37, 39, 41, 43, 50, 52, 55,  68, 69, 70, 71, 72, 73, 76]\n",
    "val_list   = [19, 21, 23, 25, 27, 29, 61, 62, 64, 65]\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "all_list = np.concatenate([train_list, val_list, test_list])\n",
    "\n",
    "for i in range(1, 77):\n",
    "    if i not in all_list:\n",
    "        continue\n",
    "    \n",
    "    #todo = [15, 16, 19, 20, 27, 29, 31, 35]\n",
    "    #allfiles = glob.glob(\"/media/drilnvm/ubuntudata2/REAL-DBT-PROJECTIONS/RECONS/*_\"+str(i)+\".raw\")\n",
    "    allfiles = glob.glob(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/RECONS-NORMAL-QUADRATIC/*_\"+str(i)+\".raw\")\n",
    "    allfiles.sort(key=lambda x: float(x.split('/')[-1].split('_')[-2]), reverse=False)\n",
    "    \n",
    "    mainfile = allfiles[0]#glob.glob(\"/media/drilnvm/ubuntudata2/REAL-DBT-PROJECTIONS/RECONS-LINEAR/*_\"+str(i)+\".raw\")[0]\n",
    "    \n",
    "    all_vols   = np.zeros([len(allfiles), 40, 1200, 3000], dtype='float16')\n",
    "    all_index  = []\n",
    "    all_values = []\n",
    "    all_values_z = []\n",
    "    \n",
    "    #temp = np.fromfile(mainfile, dtype='float32')\n",
    "    #temp = np.reshape(temp, [40, 1200, 3000])\n",
    "    #all_vols[0, :, :, :] = temp\n",
    "    #all_index.append(i)\n",
    "    #all_values.append(-0)\n",
    "    \n",
    "    counter = 0\n",
    "    for f in allfiles:\n",
    "        a = np.fromfile(f, dtype='float32')\n",
    "        a = np.reshape(a, [40, 1200, 3000]).astype('float16')\n",
    "        #a     = np.load(f)\n",
    "        #a     = np.reshape(a, [40, 1200, 3000])\n",
    "        all_vols[counter, :, :, :] = a\n",
    "        \n",
    "        index = int(f.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "        value = float(f.split(\"/\")[-1].split(\"_\")[-2])\n",
    "        \n",
    "        all_index.append(index)\n",
    "        all_values.append(value)\n",
    "        #all_values_z.append()\n",
    "        \n",
    "        print(index, value, a.shape, f)\n",
    "        counter = counter+1\n",
    "    \n",
    "    total_count   = 50\n",
    "    all_locations = []\n",
    "    \n",
    "    print('Main file: ', mainfile)\n",
    "    \n",
    "    # Get all Locations\n",
    "    while(True):\n",
    "        ix = np.random.randint(256, 1200-256)\n",
    "        iy = np.random.randint(256, 3000-256)\n",
    "        iz = np.random.randint(5, 35)\n",
    "        \n",
    "        tempx = all_vols[0][iz, ix:ix+256, iy:iy+256]\n",
    "        \n",
    "        if np.count_nonzero(tempx.flatten())*1.0/(256*256) < 0.9:\n",
    "            continue\n",
    "        \n",
    "        all_locations.append([ix, iy, iz])\n",
    "        if len(all_locations) == total_count:\n",
    "            break\n",
    "    \n",
    "    # Get patches\n",
    "    for k in range(len(all_vols)):\n",
    "        y_array = np.zeros([total_count, 256, 256, 1], dtype='float16')\n",
    "        counter = 0\n",
    "        \n",
    "        all_values_z = []\n",
    "        for p in all_locations:\n",
    "            iz = p[2]\n",
    "            ix = p[0]\n",
    "            iy = p[1]\n",
    "            \n",
    "            all_values_z.append(iz)\n",
    "            tempy   = all_vols[k][iz, ix:ix+256, iy:iy+256]\n",
    "            y_array[counter, :, :, 0] = tempy\n",
    "            counter = counter+1\n",
    "        np.save(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(all_index[k])+'_'+str(all_values[k])+'.npy', y_array)\n",
    "        np.save(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/zvalue-\"+str(all_index[k])+'_'+str(all_values[k])+'.npy',   all_values_z)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For reading the training data\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "#train_list = [1, 3, 6, 7, 11, 12, 13, 16, 17, 31, 33, 35, 37, 39, 41, 43, 50, 52, 55,  68, 69, 70, 71, 72, 73, 76]\n",
    "train_list = [1, 3, 6, 7, 11, 12, 13, 16]#, 17, 31, 33, 35, 37, 39]\n",
    "val_list   = [19, 21, 23, 25, 27, 29, 61, 62, 64, 65]\n",
    "test_list  = [10, 44, 45, 47, 54, 58, 59, 60, 66, 67, 74, 75]\n",
    "\n",
    "# 10650, 3950\n",
    "# 8200, 3160\n",
    "# 10250, 3950\n",
    "\n",
    "#11700\n",
    "#10400\n",
    "train_size = 3200\n",
    "x_array = np.zeros([train_size, 1, 256, 256], dtype='float16')\n",
    "z_array = np.zeros([train_size, 1], dtype='float16')\n",
    "v_array = np.zeros([train_size, 1], dtype='float16')\n",
    "y_array = np.zeros([train_size, 1, 256, 256], dtype='float16')\n",
    "\n",
    "# 4500\n",
    "# 3950\n",
    "val_size = 4000\n",
    "x_val_array = np.zeros([val_size, 1, 256, 256], dtype='float16')\n",
    "z_val_array = np.zeros([val_size, 1], dtype='float16')\n",
    "v_val_array = np.zeros([val_size, 1], dtype='float16')\n",
    "y_val_array = np.zeros([val_size, 1, 256, 256], dtype='float16')\n",
    "\n",
    "count = 0\n",
    "for t in train_list:\n",
    "    #print(t)\n",
    "    maintemp = np.load(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_-0.0.npy\")\n",
    "    #maintemp = np.load(mainfile)\n",
    "    \n",
    "    allfiles = glob.glob(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_*.npy\")\n",
    "    for f in allfiles:\n",
    "        if '-0.0.npy' in f:\n",
    "            continue\n",
    "        temp  = np.load(f)\n",
    "        temp1 = np.load(f.replace('imgarray', 'zvalue')) \n",
    "        #print(temp.shape)\n",
    "        print(f, temp.shape, float(f.split(\"/\")[-1].split(\"_\")[1][:-4]))\n",
    "        y_array[count: count+50, 0, :, :] = temp[:, :, :, 0]\n",
    "        x_array[count: count+50, 0, :, :] = maintemp[:, :, :, 0]\n",
    "        z_array[count: count+50, :]       = -np.ones([50, 1])*float(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "        v_array[count: count+50, :]       = np.expand_dims(temp1, axis=1)\n",
    "        count = count+50\n",
    "\n",
    "print(x_array.shape, y_array.shape, z_array.shape, count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for t in val_list:\n",
    "    #print(t)\n",
    "    maintemp = np.load(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_-0.0.npy\")\n",
    "    #maintemp = np.load(mainfile)\n",
    "    \n",
    "    allfiles = glob.glob(\"/media/drilnvm/ubuntudata1/REAL-DBT-PROJECTIONS/PATCHES-NEW-NORMAL/imgarray-\"+str(t)+\"_*.npy\")\n",
    "    for f in allfiles:\n",
    "        if '-0.0.npy' in f:\n",
    "            continue\n",
    "        temp  = np.load(f)\n",
    "        temp1 = np.load(f.replace('imgarray', 'zvalue'))\n",
    "        \n",
    "        print(f, temp.shape, float(f.split(\"/\")[-1].split(\"_\")[1][:-4]))\n",
    "        y_val_array[count: count+50, 0, :, :] = temp[:50, :, :, 0]\n",
    "        x_val_array[count: count+50, 0, :, :] = maintemp[:50, :, :, 0]\n",
    "        z_val_array[count: count+50, :]       = -np.ones([50, 1])*float(f.split(\"/\")[-1].split(\"_\")[1][:-4])\n",
    "        v_val_array[count: count+50, :]           = np.expand_dims(temp1, axis=1)\n",
    "        count = count+50\n",
    "        \n",
    "print(x_val_array.shape, y_val_array.shape, z_val_array.shape, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Rating CNN pytorch\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(rating_cnn.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min = 1000\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    rating_cnn.train()\n",
    "    loss_array = []\n",
    "    \n",
    "    for i in range(len(trainx)//32):\n",
    "        x = trainx[i*32:(i+1)*32, :, :, :]\n",
    "        y = trainy[i*32:(i+1)*32, :]\n",
    "        #z = z_array[i*8:(i+1)*8, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        #z = torch.tensor(z, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = rating_cnn(x)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape, y.data.shape)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i %100 == 0:\n",
    "            rating_cnn.eval()\n",
    "            loss_array_val = []\n",
    "            for ik in range(len(valx)//8):\n",
    "                x = valx[ik*8:(ik+1)*8, :, :, :]\n",
    "                y = valy[ik*8:(ik+1)*8, :]\n",
    "                #z = z_val_array[i*8:(i+1)*8, :]\n",
    "\n",
    "                x = torch.tensor(x, device=device).float()\n",
    "                y = torch.tensor(y, device=device).float()\n",
    "                #z = torch.tensor(z, device=device).float()\n",
    "\n",
    "                output = rating_cnn(x)\n",
    "\n",
    "                loss = criterion(output, y)\n",
    "                loss_array_val.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(loss_array_val)\n",
    "            print(\"Val loss \", val_loss)\n",
    "\n",
    "            if val_loss < prev_min:\n",
    "                prev_min = val_loss\n",
    "                torch.save(rating_cnn.state_dict(), \"rating_pytorch.pt\")\n",
    "            rating_cnn.train()\n",
    "    print(np.mean(loss_array))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result with volume Slice Huber\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.1] = []\n",
    "    ssim_array[0.2] = []\n",
    "    ssim_array[0.3] = []\n",
    "    ssim_array[0.4] = []\n",
    "    ssim_array[0.5] = []\n",
    "    ssim_array[0.6] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        \n",
    "        if value < 0.1:\n",
    "            ssim_array[0.1].append(ssim_temp)\n",
    "            #mae_array[0.1].append(mae_temp)\n",
    "        elif value < 0.2:\n",
    "            ssim_array[0.2].append(ssim_temp)\n",
    "            #mae_array[0.2].append(mae_temp)    \n",
    "        elif  value < 0.3:\n",
    "            ssim_array[0.3].append(ssim_temp)\n",
    "            #mae_array[0.3].append(mae_temp)    \n",
    "        elif  value < 0.4:\n",
    "            ssim_array[0.4].append(ssim_temp)\n",
    "            #mae_array[0.4].append(mae_temp)    \n",
    "        elif value < 0.5:\n",
    "            ssim_array[0.5].append(ssim_temp)\n",
    "            #mae_array[0.5].append(mae_temp)    \n",
    "        elif value < 0.6:\n",
    "            ssim_array[0.6].append(ssim_temp)\n",
    "            #mae_array[0.6].append(mae_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 3)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()#_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 8\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    \n",
    "    ssim_z_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    z_array = z_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss1  = my_loss(output, y)\n",
    "        #loss2  = mse_criterion(output, y)\n",
    "        \n",
    "        loss   = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    \n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss1 = my_loss(output, y)\n",
    "        #loss2 = mse_criterion(output, y)\n",
    "        loss  = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "        \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss, ' SSIM ', ssim_string)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-quadratic.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     22,
     154
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result with volume Slice Quadratic Linear\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.025] = []\n",
    "    ssim_array[0.050] = []\n",
    "    ssim_array[0.075] = []\n",
    "    ssim_array[0.100] = []\n",
    "    ssim_array[0.125] = []\n",
    "    ssim_array[0.150] = []\n",
    "    ssim_array[0.175] = []\n",
    "    ssim_array[0.200] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        \n",
    "        #print('value is ', value)\n",
    "        if np.isclose(-value, 0.025, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.025].append(ssim_temp)\n",
    "            #mae_array[0.1].append(mae_temp)\n",
    "        elif np.isclose(-value, 0.050, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.050].append(ssim_temp)\n",
    "            #mae_array[0.2].append(mae_temp)    \n",
    "        elif  np.isclose(-value, 0.075, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.075].append(ssim_temp)\n",
    "            #mae_array[0.3].append(mae_temp)    \n",
    "        elif  np.isclose(-value, 0.100, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.100].append(ssim_temp)\n",
    "            #mae_array[0.4].append(mae_temp)    \n",
    "        elif np.isclose(-value, 0.125, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.125].append(ssim_temp)\n",
    "            #mae_array[0.5].append(mae_temp)    \n",
    "        elif np.isclose(-value, 0.150, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.150].append(ssim_temp)\n",
    "        elif np.isclose(-value, 0.175, rtol=1e-02, atol=1e-02, equal_nan=False):\n",
    "            ssim_array[0.175].append(ssim_temp)\n",
    "        else:\n",
    "            ssim_array[0.200].append(ssim_temp)\n",
    "            #mae_array[0.6].append(mae_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 3)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()#_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 8\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    \n",
    "    ssim_z_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    z_array = z_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss1  = my_loss(output, y)\n",
    "        #loss2  = mse_criterion(output, y)\n",
    "        \n",
    "        loss   = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    \n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss1 = my_loss(output, y)\n",
    "        #loss2 = mse_criterion(output, y)\n",
    "        loss  = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "        \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss, ' SSIM ', ssim_string)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-quadratic-linear.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     22,
     99,
     154
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result with volume Slice Quadratic Normal\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.025] = []\n",
    "    ssim_array[0.050] = []\n",
    "    ssim_array[0.075] = []\n",
    "    ssim_array[0.100] = []\n",
    "    ssim_array[0.125] = []\n",
    "    ssim_array[0.150] = []\n",
    "    ssim_array[0.175] = []\n",
    "    ssim_array[0.200] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        \n",
    "        #print('value is ', value)\n",
    "        if -value < 0.025:\n",
    "            ssim_array[0.025].append(ssim_temp)\n",
    "            #mae_array[0.1].append(mae_temp)\n",
    "        elif -value < 0.050:\n",
    "            ssim_array[0.050].append(ssim_temp)\n",
    "            #mae_array[0.2].append(mae_temp)    \n",
    "        elif -value < 0.075:\n",
    "            ssim_array[0.075].append(ssim_temp)\n",
    "            #mae_array[0.3].append(mae_temp)    \n",
    "        elif  -value < 0.100:\n",
    "            ssim_array[0.100].append(ssim_temp)\n",
    "            #mae_array[0.4].append(mae_temp)    \n",
    "        elif -value < 0.125:\n",
    "            ssim_array[0.125].append(ssim_temp)\n",
    "            #mae_array[0.5].append(mae_temp)    \n",
    "        elif -value < 0.150:\n",
    "            ssim_array[0.150].append(ssim_temp)\n",
    "        elif -value < 0.175:\n",
    "            ssim_array[0.175].append(ssim_temp)\n",
    "        else:\n",
    "            ssim_array[0.200].append(ssim_temp)\n",
    "            #mae_array[0.6].append(mae_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 3)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()#_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 8\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    \n",
    "    ssim_z_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    z_array = z_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss1  = my_loss(output, y)\n",
    "        #loss2  = mse_criterion(output, y)\n",
    "        \n",
    "        loss   = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    \n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss1 = my_loss(output, y)\n",
    "        #loss2 = mse_criterion(output, y)\n",
    "        loss  = loss1#+10*loss2\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        z      = -1*z.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in z:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "        \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss, ' SSIM ', ssim_string)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-quadratic-normal-quarter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     56
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Pytorch Model for Imitating the result\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "model = MyUnet()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "prev_min   = 1000\n",
    "batch_size = 4\n",
    "\n",
    "#model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        #print(x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, z)\n",
    "        #print(output.shape)\n",
    "        #break\n",
    "        #print(x.data.shape, output.data.shape)\n",
    "        \n",
    "        loss = my_loss(output, y)\n",
    "        #if i % 100 == 0:\n",
    "        #    print(i, loss.data.shape, loss.item())\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        #print(loss.item())\n",
    "        #optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(np.mean(loss_array))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        z = z_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        z = torch.tensor(z, device=device).float()\n",
    "\n",
    "        output = model(x, z)\n",
    "\n",
    "        loss = my_loss(output, y)\n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    val_loss = np.mean(loss_array)\n",
    "    print(\"Val loss \", val_loss)\n",
    "    \n",
    "    if val_loss < prev_min:\n",
    "        prev_min = val_loss\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unet_pytorch_new-3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
